{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AAQAD_AutomaticGenerator.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["qA6k1REIVEWR","4ugl1V6rH0a_","MzHRGXXsTywf","vFUJ_SczUkWa","Mk8Ct2mpYf-r","QhJB77ySVU5R","ov2x7TlZYoTt","uXmIEmDIY0e2","ux7McW7AdaxZ","6gxprIQVgQoE","i3QgPHbeimuk","9VR2SngthMzv","w1knHpmsf3gn","mpw2-KnMSu59","ZLbtPghbb2Cr","mgINRGpOY-dk","7i8RCfutdiZV","UEdbfW42LfMY"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"qA6k1REIVEWR","colab_type":"text"},"source":["# Description + Algorithm"]},{"cell_type":"markdown","metadata":{"id":"KGIGbWKqYYsL","colab_type":"text"},"source":["---\n","\n","**This is  AAQAD : Alexu Arabic Question-Answer Dataset**\n","\n","**Data collection procedure is almost same as SQUAD 2.0 dataset**\n","\n","**The main objective of this Dataset is to answer MRQA Problem in Arabic**\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"q3iy7dTpDpN2","colab_type":"text"},"source":["                                            ALGORITHM  \n","            \n","            For each article in SQUAD 2.0:\n","\n","                 Open article’s wikipedia English page.\n","\n","                 If an Arabic version of this page exists:\n","\n","                            Translate English page using Google Translate\n","\n","                            Find matching translated paragraphs with the Arabic page\n","\n","                            For each matched paragraph in each article:\n","\n","                                      Save this paragraph (Arabic version from Arabic Wikipedia page)\n","\n","                                      For each Question in SQUAD 2.0 on this paragraph:\n","\n","                                                  Translate it with its answer(s) using Google Translate\n","\n","                                                  Save it with the corresponding paragraph in JSON format\n","\n","                 Else:\n","                            Abort this article (will be not included in AAQAD )\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4ugl1V6rH0a_","colab_type":"text"},"source":["# Imports + Packages install"]},{"cell_type":"code","metadata":{"id":"eKTn8yw1Rkj-","colab_type":"code","outputId":"c4b2a19c-84e5-42e4-e29d-9395370b8add","executionInfo":{"status":"ok","timestamp":1566339079378,"user_tz":-120,"elapsed":12095,"user":{"displayName":"Adel Meleka","photoUrl":"https://lh4.googleusercontent.com/-5_jHecKfhS8/AAAAAAAAAAI/AAAAAAAAAJI/0EZ1oX5bpGc/s64/photo.jpg","userId":"09805447388762865418"}},"colab":{"base_uri":"https://localhost:8080/","height":272}},"source":["!pip install googletrans"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting googletrans\n","  Downloading https://files.pythonhosted.org/packages/fd/f0/a22d41d3846d1f46a4f20086141e0428ccc9c6d644aacbfd30990cf46886/googletrans-2.4.0.tar.gz\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from googletrans) (2.21.0)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->googletrans) (2.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->googletrans) (2019.6.16)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->googletrans) (3.0.4)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->googletrans) (1.24.3)\n","Building wheels for collected packages: googletrans\n","  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for googletrans: filename=googletrans-2.4.0-cp36-none-any.whl size=15776 sha256=d765345c911f2754a155091923eb5b7afbee0f3610c516c93838302280cbd131\n","  Stored in directory: /root/.cache/pip/wheels/50/d6/e7/a8efd5f2427d5eb258070048718fa56ee5ac57fd6f53505f95\n","Successfully built googletrans\n","Installing collected packages: googletrans\n","Successfully installed googletrans-2.4.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eBpXnOtfi9LD","colab_type":"code","colab":{}},"source":["import requests\n","from bs4 import BeautifulSoup\n","import unicodedata\n","from googletrans import Translator\n","import difflib\n","import json\n","import re\n","import numpy as np\n","from nltk import ngrams\n","from nltk import TreebankWordTokenizer\n","from nltk import WordPunctTokenizer\n","from nltk import WhitespaceTokenizer\n","from textblob.base import BaseTokenizer\n","import gensim\n","from scipy import spatial\n","from nltk import ngrams\n","import operator"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MzHRGXXsTywf","colab_type":"text"},"source":["# Implementation Functions"]},{"cell_type":"markdown","metadata":{"id":"vFUJ_SczUkWa","colab_type":"text"},"source":["## **Retrieve Arabic's Wikipedia paragarphs (if exists)**"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"iRKtagZ-MnjS","colab":{}},"source":["##Given article title from SQUAD 2.0\n","#Check if arabic page exists and retrieve its arabic paragraphs\n","\n","#returns a boolean for arabic page existance\n","#and the arabic title\n","#and the arabic paragraphs\n","\n","def get_arabic_paragraphs(title):\n","  \n","  arabic_page_exists = True\n","  \n","  html = requests.get('https://ar.wikipedia.org/wiki/'+title).text\n","  soup = BeautifulSoup(html, \"html.parser\")\n","  ar_paragraphs = [p.get_text() for p in soup.find_all(\"p\")]\n","   \n","  #check if arabic wikipedia page do not exists for a given title\n","  if len(ar_paragraphs) == 1 and \"هذه الصفحة خالية\" in ar_paragraphs[0]:\n","    \n","    arabic_page_exists = False  \n","   \n","  #if arabic wikepidia page exists \n","  else:\n","    \n","    #get arabic title \n","    ar_title = [h1.get_text() for h1 in soup.find_all(\"h1\")]\n","    ar_title =  ar_title[0]\n","    \n","    #reformat arabic paragraphs\n","    for i in range(len(ar_paragraphs)):\n","      ar_paragraphs[i] = unicodedata.normalize(\"NFD\", ar_paragraphs[i])\n","      ar_paragraphs[i] = re.sub(r'(\\[(\\d+)\\])|(\\[بحاجة لمصدر\\])', '', ar_paragraphs[i])\n","            \n","  return arabic_page_exists, ar_title, ar_paragraphs"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mk8Ct2mpYf-r","colab_type":"text"},"source":["### **Testing**"]},{"cell_type":"code","metadata":{"id":"fRrdYXxYNx6z","colab_type":"code","outputId":"44fb69b0-0b16-4433-e795-bbd5da91526e","executionInfo":{"status":"ok","timestamp":1566339085435,"user_tz":-120,"elapsed":1133,"user":{"displayName":"Adel Meleka","photoUrl":"https://lh4.googleusercontent.com/-5_jHecKfhS8/AAAAAAAAAAI/AAAAAAAAAJI/0EZ1oX5bpGc/s64/photo.jpg","userId":"09805447388762865418"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#testing\n","\n","#check, ar_parag =  get_arabic_paragraphs(\"Wikipedia:Articles_in_many_other_languages_but_not_on_English_Wikipedia\")\n","\n","check, ar_title, ar_parag = get_arabic_paragraphs(\"Beyoncé\")\n","\n","print(ar_title)\n","# print(len(ar_parag))\n","\n","# for p in ar_parag:\n","#   print(p)\n","#   print(\"##########################################\")\n","\n","# print(\"##########################################\")\n","# for p in en_parag:\n","#   print(p)\n","#   print(\"##########################################\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["بيونسيه\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QhJB77ySVU5R","colab_type":"text"},"source":["## Translate Arabic paragraphs"]},{"cell_type":"code","metadata":{"id":"4X0FqMg-Y_w0","colab_type":"code","colab":{}},"source":["## dictionary to convert arabic digits to english\n","\n","eastern_to_western = {\"٠\":\"0\",\"١\":\"1\",\"٢\":\"2\",\"٣\":\"3\",\"٤\":\"4\",\"٥\":\"5\",\"٦\":\"6\",\"٧\":\"7\",\"٨\":\"8\",\"٩\":\"9\",\n","                      \"0\":\"0\",\"1\":\"1\",\"2\":\"2\",\"3\":\"3\",\"4\":\"4\",\"5\":\"5\",\"6\":\"6\",\"7\":\"7\",\"8\":\"8\",\"9\":\"9\"}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oXW8dsHwOWCs","colab_type":"code","colab":{}},"source":["## translate a given paragraph to arabic\n","\n","def translate_to_arabic(paragraph):\n","  \n","  translator = Translator()\n","  translatedParagraph = translator.translate(paragraph, dest='ar')\n","  \n","  #replace arabic digits with english ones\n","  translatedParagraph = list(translatedParagraph.text)\n","   \n","  for i in range(0,len(translatedParagraph)):\n","    if translatedParagraph[i].isdigit():\n","      translatedParagraph[i] = eastern_to_western[translatedParagraph[i]] \n","\n","  translatedParagraph = \"\".join(translatedParagraph)\n","  \n","  return translatedParagraph"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ov2x7TlZYoTt","colab_type":"text"},"source":["### **Testing**"]},{"cell_type":"code","metadata":{"id":"5I_PhyGTa0bV","colab_type":"code","outputId":"d10124a0-0b5e-4256-80c2-da4d3425b21a","executionInfo":{"status":"ok","timestamp":1566217643164,"user_tz":-120,"elapsed":912,"user":{"displayName":"Adel Meleka","photoUrl":"https://lh4.googleusercontent.com/-5_jHecKfhS8/AAAAAAAAAAI/AAAAAAAAAJI/0EZ1oX5bpGc/s64/photo.jpg","userId":"09805447388762865418"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["## testing\n","\n","p = \"Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say; born September 4, 1981)[4] is an American singer, songwriter and actress. Born and raised in Houston, Texas, Beyoncé performed in various singing and dancing competitions as a child. She rose to fame in the late 1990s as lead singer of the R&B girl-group Destiny's Child, one of the best-selling girl groups in history. Their hiatus saw the release of her first solo album, Dangerously in Love (2003), which debuted at number one on the US Billboard 200 chart and earned her five Grammy Awards.[5] The album also featured the US Billboard Hot 100 number-one singles 'Crazy in Love' and 'Baby Boy'.\"\n","\n","# p = \"Beyoncé Giselle Knowl 20\"\n","\n","translate_to_arabic(p)\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'بيونسي جيزيل نولز كارتر (/ biːˈjɒnseɪ / bee-YON- say ؛ من مواليد 4 سبتمبر 1981) [4] مغنية وكاتب أغاني وممثلة أمريكية. ولدت ونشأت في هيوستن ، تكساس ، بيونسيه في مختلف مسابقات الغناء والرقص كطفل. صعدت إلى الشهرة في أواخر التسعينيات كمغنية رئيسية لمجموعة ديستينيز تشايلد للبنات ، إحدى أفضل مجموعات الفتيات مبيعًا في التاريخ. شهدت الفجوة الخاصة بهم إصدار ألبومها الفردي الأول ، Dangerious in Love (2003) ، الذي ظهر لأول مرة في المرتبة الأولى على مخطط Billboard 200 الأمريكي وحاز على جوائز Grammy الخمس. [5] وضم الألبوم أيضًا أغنية US Billboard Hot 100 الفردية الأولى \"Crazy in Love\" و \"Baby Boy\".'"]},"metadata":{"tags":[]},"execution_count":88}]},{"cell_type":"code","metadata":{"id":"kLMQdsYY2Ij-","colab_type":"code","outputId":"9d772571-8be5-45c1-fe9c-797ac57cd426","executionInfo":{"status":"ok","timestamp":1566202046719,"user_tz":-120,"elapsed":1423,"user":{"displayName":"Adel Meleka","photoUrl":"https://lh4.googleusercontent.com/-5_jHecKfhS8/AAAAAAAAAAI/AAAAAAAAAJI/0EZ1oX5bpGc/s64/photo.jpg","userId":"09805447388762865418"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["## testing arabic to english digit conversion\n","\n","s = \" حكم الدولة العثمانية من سنة ١١١١ حتى سنة ٣٣٣٣\"\n","s = list(s)\n","\n","for i in range(0,len(s)):\n","  if s[i].isdigit():\n","    s[i] = ''.join([eastern_to_western[c] for c in s[i]])\n","    \n","s = \"\".join(s)\n","\n","print(s)"],"execution_count":0,"outputs":[{"output_type":"stream","text":[" حكم الدولة العثمانية من سنة 1111 حتى سنة 3333\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uXmIEmDIY0e2","colab_type":"text"},"source":["## **Similarity Pretrained Model**"]},{"cell_type":"markdown","metadata":{"id":"UudQa98qrh2v","colab_type":"text"},"source":["**This Source Code is taken from:**\n","1. https://github.com/bakrianoo/aravec\n","2. https://github.com/adhaamehab/textblob-ar"]},{"cell_type":"markdown","metadata":{"id":"ux7McW7AdaxZ","colab_type":"text"},"source":["### **Download Pretrained Word Embedding Model**"]},{"cell_type":"code","metadata":{"id":"d-OgzgZwb84O","colab_type":"code","colab":{}},"source":["### downloading a pretrained arabic word embedding model\n","!wget -qq https://bakrianoo.sfo2.digitaloceanspaces.com/aravec/full_grams_cbow_300_wiki.zip -P ./data\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GJ-uBs3YgqZ0","colab_type":"code","colab":{}},"source":["### unzip the pretrained model  \n","!unzip -qq data/full_grams_cbow_300_wiki.zip -d ./data"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6gxprIQVgQoE","colab_type":"text"},"source":["### **utilities.py Module**"]},{"cell_type":"code","metadata":{"id":"K6uQKloDgVAE","colab_type":"code","colab":{}},"source":["# =========================\n","# ==== Helper Methods =====\n","\n","# Clean/Normalize Arabic Text\n","def clean_str(text):\n","    search = [\"أ\",\"إ\",\"آ\",\"ة\",\"_\",\"-\",\"/\",\".\",\"،\",\" و \",\" يا \",'\"',\"ـ\",\"'\",\"ى\",\"\\\\\",'\\n', '\\t','&quot;','?','؟','!']\n","    replace = [\"ا\",\"ا\",\"ا\",\"ه\",\" \",\" \",\"\",\"\",\"\",\" و\",\" يا\",\"\",\"\",\"\",\"ي\",\"\",' ', ' ',' ',' ? ',' ؟ ',' ! ']\n","    \n","    #remove tashkeel\n","    p_tashkeel = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n","    text = re.sub(p_tashkeel,\"\", text)\n","    \n","    #remove longation\n","    p_longation = re.compile(r'(.)\\1+')\n","    subst = r\"\\1\\1\"\n","    text = re.sub(p_longation, subst, text)\n","    \n","    text = text.replace('وو', 'و')\n","    text = text.replace('يي', 'ي')\n","    text = text.replace('اا', 'ا')\n","    \n","    for i in range(0, len(search)):\n","        text = text.replace(search[i], replace[i])\n","    \n","    #trim    \n","    text = text.strip()\n","\n","    return text\n","\n","def get_vec(n_model,dim, token):\n","    vec = np.zeros(dim)\n","    exist = False\n","    is_vec = False\n","    if token not in n_model.wv:\n","        _count = 0\n","        is_vec = True\n","        for w in token.split(\"_\"):\n","            if w in n_model.wv:\n","                _count += 1\n","                vec += n_model.wv[w]\n","        if _count > 0:\n","            vec = vec / _count\n","            exist = True\n","    else:\n","        vec = n_model.wv[token]\n","        exist = True\n","    \n","    return vec,exist\n","\n","def calc_vec(pos_tokens, neg_tokens, n_model, dim):\n","    vec = np.zeros(dim)\n","    for p in pos_tokens:\n","        vec += get_vec(n_model,dim,p)\n","    for n in neg_tokens:\n","        vec -= get_vec(n_model,dim,n)\n","    \n","    return vec   \n","\n","## -- Retrieve all ngrams for a text in between a specific range\n","def get_all_ngrams(text, nrange=3):\n","    text = re.sub(r'[\\,\\.\\;\\(\\)\\[\\]\\_\\+\\#\\@\\!\\?\\؟\\^]', ' ', text)\n","    tokens = [token for token in text.split(\" \") if token.strip() != \"\"]\n","    ngs = []\n","    for n in range(2,nrange+1):\n","        ngs += [ng for ng in ngrams(tokens, n)]\n","    return [\"_\".join(ng) for ng in ngs if len(ng)>0 ]\n","\n","## -- Retrieve all ngrams for a text in a specific n\n","def get_ngrams(text, n=2):\n","    text = re.sub(r'[\\,\\.\\;\\(\\)\\[\\]\\_\\+\\#\\@\\!\\?\\؟\\^]', ' ', text)\n","    tokens = [token for token in text.split(\" \") if token.strip() != \"\"]\n","    ngs = [ng for ng in ngrams(tokens, n)]\n","    return [\"_\".join(ng) for ng in ngs if len(ng)>0 ]\n","\n","## -- filter the existed tokens in a specific model\n","def get_existed_tokens(tokens, n_model):\n","    return [tok for tok in tokens if tok in n_model.wv ]\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i3QgPHbeimuk","colab_type":"text"},"source":["### **tokenizer.py Module**"]},{"cell_type":"code","metadata":{"id":"R18iBlqNir2N","colab_type":"code","colab":{}},"source":["class NLTKTreebankWordTokenizer(BaseTokenizer):\n","\n","    def tokenize(self, text):\n","        return TreebankWordTokenizer().tokenize(text)\n","\n","class NLTKWordPunctTokenizer(BaseTokenizer):\n","\n","    def tokenize(self, text):\n","        return WordPunctTokenizer().tokenize(text)\n","\n","\n","class NLTKWhitespaceTokenizer(BaseTokenizer):\n","\n","    def tokenize(self, text):\n","        return WhitespaceTokenizer().tokenize(text)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9VR2SngthMzv","colab_type":"text"},"source":["### **similarity.py Module**"]},{"cell_type":"code","metadata":{"id":"WZR4AbBNhMJH","colab_type":"code","colab":{}},"source":["\n","class TextSimilarity:\n","\n","    def __init__(self):\n","        try:\n","            self.model = gensim.models.Word2Vec.load('data/full_grams_cbow_300_wiki.mdl')\n","        except FileNotFoundError:\n","            raise FileNotFoundError\n","            \n","    def avg_feature_vector(self, sentence, num_features=300):\n","        words = NLTKWordPunctTokenizer().tokenize(clean_str(sentence))\n","        feature_vec = np.zeros((num_features, ), dtype='float32')\n","        n_words = 0\n","        for word in words:\n","            word_vect,exist = get_vec(n_model=self.model, dim=num_features, token=word)\n","            feature_vec = np.add(feature_vec, word_vect)\n","            if exist:\n","              n_words += 1\n","        if (n_words > 0):\n","            feature_vec = np.divide(feature_vec, n_words)\n","        return feature_vec\n","\n","    def similarity(self, sentence1, sentence2):\n","        vec1, vec2 = self.avg_feature_vector(sentence1), self.avg_feature_vector(sentence2)\n","        return self.cosine_similarity(vec1, vec2)\n","\n","    def cosine_similarity(self, vec1, vec2):\n","        return 1 - spatial.distance.cosine(vec1, vec2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w1knHpmsf3gn","colab_type":"text"},"source":["### **Build Model**"]},{"cell_type":"code","metadata":{"id":"XS0OhM8PwGiW","colab_type":"code","outputId":"169fe281-a102-4078-b8d9-d350039b18cd","executionInfo":{"status":"ok","timestamp":1566339181032,"user_tz":-120,"elapsed":5277,"user":{"displayName":"Adel Meleka","photoUrl":"https://lh4.googleusercontent.com/-5_jHecKfhS8/AAAAAAAAAAI/AAAAAAAAAJI/0EZ1oX5bpGc/s64/photo.jpg","userId":"09805447388762865418"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["sim = TextSimilarity()\n","# takes around 12 second (macbook pro 2017) to load the pretrained word2vec"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"n20ahM_QmYHH","colab_type":"code","outputId":"14a413f8-4c19-419f-eecb-f482978f268b","executionInfo":{"status":"ok","timestamp":1566128896399,"user_tz":-120,"elapsed":1063,"user":{"displayName":"Adel Meleka","photoUrl":"https://lh4.googleusercontent.com/-5_jHecKfhS8/AAAAAAAAAAI/AAAAAAAAAJI/0EZ1oX5bpGc/s64/photo.jpg","userId":"09805447388762865418"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["## testing similarity\n","\n","sent1 = u'الإرهابي الصالح هي رواية خيال سياسي للكاتبة دوريس ليسينج. ظهرت أول طبعة للرواية في سبتمبر من عام 1985 للناشرين جوناثان كيب في المملكة المتحدة وألفريد أ'\n","\n","# sent2 = u'روايه الكاتبه دوريس ليسينج هي روايه خيال سياسي ظهرت في سبتمبر 1985 بعنوان الارهابي الصالح وتم نشرها عن طريق جوناثان كيب والفريد أ في انجلترا'\n","\n","sent2 = u'الكوكبة هي مجموعة من النجوم التي تكون شكلا أو صورة، وهي تدل على المنطقة التي تظهر فيها مجموعة محدودة من النجوم. وقد قسم الاتحاد الفلكي الدولي في عام 1930 السماء إلى 88 كوكبة، وذلك لتوحيد أشكال الكوكبات وعددها بعد أن كانت تتخيلها كل من الحضارات القديمة بشكل مختلف.'\n","\n","\n","sim.similarity(sent1, sent2)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.34852882960983866"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"mpw2-KnMSu59","colab_type":"text"},"source":["### **Difflib built-in Similarity Method**"]},{"cell_type":"code","metadata":{"id":"RSXeFeJUCqHw","colab_type":"code","colab":{}},"source":["def difflib_similarity(paragraph1, paragraph2):\n","  \n","  sequence = difflib.SequenceMatcher(a = paragraph1, b = paragraph2, autojunk= False)\n","  difference = sequence.ratio()\n","  \n","  return difference  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZLbtPghbb2Cr","colab_type":"text"},"source":["## **Retrieve Similar Arabic Paragraphs**"]},{"cell_type":"code","metadata":{"id":"TXs4kwpZRF-7","colab_type":"code","colab":{}},"source":["## Given a translated arabic paragraph it is\n","#  required to retieve the most similar araibc\n","#  paragraphs from all wikipedia arabic paragraphs\n","\n","## return:\n","#  boolean to check if the corresponding arabic parag exists\n","#  text containing the correct retrieved arabic paragraph\n","\n","def get_similar_ar_paragraph(translated_ar_paragraph, ar_paragraphs):\n","  \n","  is_similar = True\n","  max_similarity = -1\n","  correct_ar_paragraph = \"\"\n","  \n","  for parag in ar_paragraphs:\n","    siml = sim.similarity(translated_ar_paragraph,parag)  \n","       \n","    if siml > max_similarity:\n","        max_similarity = siml\n","        correct_ar_paragraph = parag\n","                               \n","  #Threshold for similarity => 80%                            \n","  if max_similarity < 0.7:\n","    is_similar = False                 \n","                              \n","                               \n","  return is_similar, correct_ar_paragraph"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mgINRGpOY-dk","colab_type":"text"},"source":["## **Finding correct answer from Arabic Paragraphs**"]},{"cell_type":"code","metadata":{"id":"EEhE_qGq5noz","colab_type":"code","colab":{}},"source":["######################### COMPARING ALL SENTENCES ###############################\n","\n","\n","### finding exact matching answer -if exists- given an arabic paragraph\n","## uses similarity function to compare translated answer with possible\n","## answer from the arabic paragraphs...\n","\n","ANS_SIM_THRESHOLD = 0.8\n","\n","def find_answer(ar_paragraph,translated_answer):\n","  \n","  ## required to return\n","  correct_answer = ''\n","  answer_exist = True\n","  answer_start_index = 0\n","  sim_dic = {}\n","  \n","  ## find most similar answer in each sentence in paragraph\n","  ar_sentences = ar_paragraph.split('.')\n","  max_sim_answer = -1\n","  for  similar_sentence in ar_sentences:\n","    ## retrieve exact similar word from similar_sentences\n","    words = similar_sentence.split(' ')\n","\n","    for i in range(0,len(words)):\n","      for j in range(i,len(words)):\n","\n","        temp_ans = ''\n","        num_words = j-i+1\n","        l = i \n","        for k in range(0,num_words):\n","          temp_ans += words[l] + ' '\n","          l += 1\n","\n","        siml = sim.similarity(translated_answer,temp_ans)\n","        if temp_ans != ' ':\n","          sim_dic[temp_ans]= siml\n","        if siml > max_sim_answer:\n","          max_sim_answer = siml\n","          correct_answer = temp_ans\n","      \n","#   print(\"correct: \",correct_answer)\n","  temp_correct_ans = correct_answer\n","  if max_sim_answer < ANS_SIM_THRESHOLD:\n","    answer_exist = False\n","  \n","  else: \n","    \n","    # compare highest 3 similarity answers with difflib\n","    sorted_x = []\n","    for i in range(0,3):\n","      sorted_x.append(max(sim_dic.items(), key=operator.itemgetter(1)))\n","      sim_dic.pop(max(sim_dic.items(), key=operator.itemgetter(1))[0], None)\n","      \n","    max_difflib = -1\n","    for e in sorted_x:\n","   \n","      if e[1] >= ANS_SIM_THRESHOLD:\n","        difflib_sim = difflib_similarity(e[0],translated_answer)\n","        if difflib_sim > max_difflib:\n","          max_difflib = difflib_sim\n","          correct_answer = e[0]\n","    \n","    if max_difflib < 0.5:\n","      correct_answer = temp_correct_ans\n","    \n","   \n","    \n","    # retrieve answer start index\n","    correct_answer = correct_answer.strip() \n","    answer_start_index = ar_paragraph.find(correct_answer)    \n","  \n","  \n","  return  answer_exist, correct_answer, answer_start_index\n","\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AGASyirUaRSb","colab_type":"code","colab":{}},"source":["\n","# ######################### NOT USED ###############################\n","\n","\n","# ### finding exact matching answer -if exists- given an arabic paragraph\n","# ## uses similarity function to compare translated answer with possible\n","# ## answer from the arabic paragraphs...\n","\n","# ANS_SIM_THRESHOLD = 0.8\n","\n","# def find_answer(ar_paragraph,translated_answer):\n","  \n","#   ## required to return\n","#   correct_answer = ''\n","#   answer_exist = True\n","#   answer_start_index = 0\n","#   sim_dic = {}\n","  \n","#   ## retrieve max similarity sentence with the answer\n","#   max_similarity = -1\n","#   similar_sentence = ''\n","#   ar_sentences = ar_paragraph.split('.')\n","  \n","#   for  s in ar_sentences:\n","#     #calculate similarity\n","#     siml = sim.similarity(translated_answer,s)\n","#     if siml > max_similarity:\n","#       similar_sentence = s\n","#       max_similarity = siml\n","\n","#   ## retrieve exact similar word from similar_sentence \n","#   max_sim_answer = -1\n","#   words = similar_sentence.split(' ')\n","  \n","#   for i in range(0,len(words)):\n","#     for j in range(i,len(words)):\n","      \n","#       temp_ans = ''\n","#       num_words = j-i+1\n","#       l = i \n","#       for k in range(0,num_words):\n","#         temp_ans += words[l] + ' '\n","#         l += 1\n","      \n","#       siml = sim.similarity(translated_answer,temp_ans)\n","#       if temp_ans != ' ':\n","#         sim_dic[temp_ans]= siml\n","#       if siml > max_sim_answer:\n","#         max_sim_answer = siml\n","#         correct_answer = temp_ans\n","      \n","  \n","#   temp_correct_ans = correct_answer\n","#   if max_sim_answer < ANS_SIM_THRESHOLD:\n","#     answer_exist = False\n","  \n","#   else: \n","    \n","#     # compare highest 3 similarity answers with difflib\n","#     sorted_x = []\n","#     for i in range(0,3):\n","#       sorted_x.append(max(sim_dic.items(), key=operator.itemgetter(1)))\n","#       sim_dic.pop(max(sim_dic.items(), key=operator.itemgetter(1))[0], None)\n","      \n","#     max_difflib = -1\n","#     for e in sorted_x:\n","   \n","#       if e[1] >= ANS_SIM_THRESHOLD:\n","#         difflib_sim = difflib_similarity(e[0],translated_answer)\n","#         if difflib_sim > max_difflib:\n","#           max_difflib = difflib_sim\n","#           correct_answer = e[0]\n","    \n","#     if max_difflib < 0.5:\n","#       correct_answer = temp_correct_ans\n","    \n","   \n","    \n","#     # retrieve answer start index\n","#     correct_answer = correct_answer.strip() \n","#     answer_start_index = ar_paragraph.find(correct_answer)    \n","  \n","  \n","#   return  answer_exist, correct_answer, answer_start_index\n","\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7i8RCfutdiZV","colab_type":"text"},"source":["### **Testing**"]},{"cell_type":"code","metadata":{"id":"ZzvbDuh1b5DW","colab_type":"code","outputId":"75a022ec-5c50-4ebe-9a3c-d0375800050f","executionInfo":{"status":"ok","timestamp":1566302872566,"user_tz":-120,"elapsed":2095,"user":{"displayName":"Adel Meleka","photoUrl":"https://lh4.googleusercontent.com/-5_jHecKfhS8/AAAAAAAAAAI/AAAAAAAAAJI/0EZ1oX5bpGc/s64/photo.jpg","userId":"09805447388762865418"}},"colab":{"base_uri":"https://localhost:8080/","height":151}},"source":["par = u\"بيونسي جيزيل نولز-كارتر (من مواليد 4 سبتمبر، 1981)، المعروفة باسم بيونسي. ولدت ونشأت في هيوستن بولاية تكساس، هي مغنية وممثلة أميركية حائزة على 23 جائزة غرامي.غنت في مسابقات غناء ورقص مختلفة عندما كانت طفلة، أصبحت مشهورة في أواخر التسعينات كمغنية آر أند بي (رئيسية) للفرقة الغنائية النسائية دستنيز تشايلد. والتي أديرت من قِبل والدها ماثيو نولز، وأصبحت الفرقة واحدة من الأكثر مبيعاً في العالم من الفرق النسائية على الإطلاق. وقد شهد إنفصال الفرقة المؤقت صدور ألبوم بيونسي الأول Dangerously in Love دانجيروسلي إن لوف (2003)، والذي أنشأها بأن تكون فنان منفرد ناجح في العالم؛ بيعت منه 16 مليون نسخة، حصل على خمسة جوائز غرامي وتضمن الأغاني التي وصلت إلى قمة الرسم البياني الأمريكي بيلبورد هوت 100 كريزي إن لوف و بيبي بوي\"\n","\n","\n","ans = u\"المعروفة باسم بيونسي\"\n","\n","# ans = \"\"\n","\n","\n","ans_exist, correct_ans, index = find_answer(par,ans)\n","\n","print(ans_exist)\n","print(correct_ans)\n","print(index)\n","\n","\n","# print(par[52],par[53],par[54],par[55])\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in double_scalars\n","  dist = 1.0 - uv / np.sqrt(uu * vv)\n","/usr/local/lib/python3.6/dist-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in float_scalars\n","  dist = 1.0 - uv / np.sqrt(uu * vv)\n"],"name":"stderr"},{"output_type":"stream","text":["correct:  المعروفة باسم بيونسي \n","True\n","المعروفة باسم بيونسي\n","52\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hlooTuDA7nUy","colab_type":"code","outputId":"e8ee3963-c668-40b3-ed9a-c3dede58c704","executionInfo":{"status":"ok","timestamp":1566230783903,"user_tz":-120,"elapsed":886,"user":{"displayName":"Adel Meleka","photoUrl":"https://lh4.googleusercontent.com/-5_jHecKfhS8/AAAAAAAAAAI/AAAAAAAAAJI/0EZ1oX5bpGc/s64/photo.jpg","userId":"09805447388762865418"}},"colab":{"base_uri":"https://localhost:8080/","height":67}},"source":["x = {\"adel\": 0.22222222222222222222222222222222222, \"sandra\": 0.444444444444444444444444444444, \"rimon\": 0.3222222222222222222222222222222222,\"hamada\":1.0}\n","\n","# sorted_x = sorted(x.items(), key=operator.itemgetter(1), reverse=True)\n","\n","# for e in sorted_x:\n","#   print(e)\n","\n","  \n","#   max(x.items(), key=operator.itemgetter(1))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["('hamada', 1.0)\n","('sandra', 0.4444444444444444)\n","('rimon', 0.32222222222222224)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UEdbfW42LfMY","colab_type":"text"},"source":["## Save data into JSON File"]},{"cell_type":"code","metadata":{"id":"Hfv0wRvhLpsY","colab_type":"code","colab":{}},"source":["def save_JSON(data):\n","  \n","  with open('./data/AAQAD-v1.0.json', 'w') as outfile:\n","    json.dump(data, outfile)\n","    \n","  return"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FmJd3itTT78n","colab_type":"text"},"source":["# Main Function [ Run Here ]"]},{"cell_type":"markdown","metadata":{"id":"g9PZ8rp8eQL4","colab_type":"text"},"source":["### **Code**"]},{"cell_type":"code","metadata":{"id":"yiC19CM-cb0J","colab_type":"code","colab":{}},"source":["### download SQUAD dataset on colab (found in 'Files/data' section)\n","\n","#training set \n","!wget -qq https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json -P ./data\n","  \n","#dev set\n","!wget -qq https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json -P ./data\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sL1bASiHUCXM","colab_type":"code","colab":{}},"source":["def AQQAD_generator():\n","  \n","  #### dictionary dataset declartion \n","  # Will be used to store final JSON format of the dataset\n","  AAQAD_dic = {}\n","  AAQAD_dic[\"version\"] = \"v1.0\"\n","  AAQAD_dic['data'] = []\n","  Question_ID = 1\n","  \n","  #### Imitate training set\n","  with open('./data/train-v2.0.json') as json_file:  \n","    file = json.load(json_file)\n","    \n","  ## loop on each article...\n","  article_count = -1\n","  for article in file[\"data\"][0:1]:\n","    article_count += 1\n","    print(\"##################################################\")\n","    print(\"Article no \",article_count, \" : \",article[\"title\"])\n","        \n","    # boolean to check if it is valid aritcle\n","    # by having at least 1 valid paragraph\n","    valid_article = False\n","\n","    #extract all arabic paragraphs(if exists) from article title\n","    arabic_page_exist, ar_title, ar_paragraphs = get_arabic_paragraphs(article[\"title\"])\n","\n","    if arabic_page_exist:\n","      print(\"Arabic page exits !\")\n","      article_paragraphs = []\n","\n","      article_initiazlized = False\n","\n","      ## loop on each paragraph for the current article... \n","      # (context section + QAS sections)\n","      parag_count = -1\n","      for parag in article[\"paragraphs\"]: \n","        parag_count += 1\n","        print(\"\\nPragraph no: \",parag_count)\n","        \n","        #translate current english paragraph in SQUAD 2.0 into arabic\n","        translated_ar_paragraph = translate_to_arabic(parag[\"context\"])\n","\n","        ## find a similarity between translated paragraph and the arabic paragraphs\n","        # Similarity must be the max & above 80 %\n","        similar_parag_exist, correct_ar_parag = get_similar_ar_paragraph(translated_ar_paragraph,ar_paragraphs)\n","        if similar_parag_exist:\n","          #remove matched arabic paragraph from ar_list\n","          #ar_paragraphs.remove(correct_ar_parag)\n","\n","          if article_initiazlized == False :\n","            AAQAD_dic['data'].append({'title' : ar_title,\n","                                      'paragraphs': []})\n","            article_initiazlized = True\n","\n","          print(\"Paragraph no: \",parag_count,\" was found in arabic\")\n","#             print(correct_ar_parag)\n","\n","          ## loop on questions for the curent paragraph\n","          print(\"Questions:\")\n","          ques_count = -1\n","          # boolean to check for at least 1 valid question for the current paragraph\n","          valid_questions = False\n","          qas = []\n","          for ques in parag[\"qas\"]:\n","            ques_count += 1\n","            print(\"\\nquestion no: \",ques_count)\n","\n","            # translate question to arabic\n","            translated_ques = translate_to_arabic(ques[\"question\"])\n","\n","            # find if first answer(or plausible answer) exist in the arabic matched paragraph\n","            print(\"Cheking Answer\")\n","\n","#             print(\"is impossible -> \",type(ques[\"is_impossible\"]))\n","            if ques[\"is_impossible\"] == False:\n","              translated_ans = translate_to_arabic(ques[\"answers\"][0][\"text\"])\n","            else:\n","              translated_ans = translate_to_arabic(ques[\"plausible_answers\"][0][\"text\"])\n","\n","            #search for the existance of the correct answer in ar paragraph\n","            ans_exist, correct_ans, ans_index_start = find_answer(correct_ar_parag,translated_ans)\n","            if ans_exist:\n","              print(\"Answer exists for Question no: \",ques_count)\n","              if valid_questions == False:\n","                valid_questions = True\n","\n","              # adding valid answer to appropiate dictionaries\n","              ans_details = {'text': correct_ans,\n","                            'answer_start':ans_index_start}\n","              if ques[\"is_impossible\"] == False:\n","                qas.append({'question' : translated_ques,\n","                            'id': Question_ID,\n","                            'answers': [ans_details],\n","                            'is_impossible': False})                  \n","              else:\n","                qas.append({'plausible_answers':[ans_details],\n","                            'question' : translated_ques,\n","                            'id': Question_ID,\n","                            'answers': [],\n","                            'is_impossible': True})\n","              Question_ID += 1  \n","                     \n","            else:\n","              print(\"Answer do not exist for Question no: \",ques_count)\n","          \n","          if valid_questions == True:\n","                valid_article = True\n","                article_paragraphs.append({'qas': qas, 'context' : correct_ar_parag })\n","                \n","\n","        else:\n","          print(\"Paragraph no: \",parag_count,\" was NOT found in arabic\")\n","\n","        print(\"--------------------------------------------------------------------\")\n","\n","        \n","      ## add all paragraphs to the correct article in AAQAD_dic\n","      if valid_article == True:\n","        AAQAD_dic['data'][article_count]['paragraphs'] = article_paragraphs\n","    \n","    else:\n","      print(\"Arabic page do NOT exist in Arabic\")\n","\n","\n","    print(\"##################################################\")\n","  \n","  \n","  #### Write prepared dataset (dic) into a JSON file\n","  save_JSON(AAQAD_dic)\n","  \n","  return"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"09CfqbK6c107","colab_type":"text"},"source":["### **Testing**"]},{"cell_type":"code","metadata":{"id":"VqZorEeJNAYv","colab_type":"code","outputId":"f0ac155d-4a9a-475b-82b0-9e5dc8e0ed9b","executionInfo":{"status":"ok","timestamp":1566339566544,"user_tz":-120,"elapsed":104787,"user":{"displayName":"Adel Meleka","photoUrl":"https://lh4.googleusercontent.com/-5_jHecKfhS8/AAAAAAAAAAI/AAAAAAAAAJI/0EZ1oX5bpGc/s64/photo.jpg","userId":"09805447388762865418"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#testing \n","\n","AQQAD_generator()\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["##################################################\n","Article no  0  :  Beyoncé\n","Arabic page exits !\n","\n","Pragraph no:  0\n","Paragraph no:  0  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  1\n","Paragraph no:  1  was found in arabic\n","Questions:\n","\n","question no:  0\n","Cheking Answer\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in double_scalars\n","  dist = 1.0 - uv / np.sqrt(uu * vv)\n","/usr/local/lib/python3.6/dist-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in float_scalars\n","  dist = 1.0 - uv / np.sqrt(uu * vv)\n"],"name":"stderr"},{"output_type":"stream","text":["Answer do not exist for Question no:  0\n","\n","question no:  1\n","Cheking Answer\n","Answer do not exist for Question no:  1\n","\n","question no:  2\n","Cheking Answer\n","Answer do not exist for Question no:  2\n","\n","question no:  3\n","Cheking Answer\n","Answer exists for Question no:  3\n","\n","question no:  4\n","Cheking Answer\n","Answer do not exist for Question no:  4\n","\n","question no:  5\n","Cheking Answer\n","Answer do not exist for Question no:  5\n","\n","question no:  6\n","Cheking Answer\n","Answer do not exist for Question no:  6\n","\n","question no:  7\n","Cheking Answer\n","Answer do not exist for Question no:  7\n","\n","question no:  8\n","Cheking Answer\n","Answer do not exist for Question no:  8\n","\n","question no:  9\n","Cheking Answer\n","Answer exists for Question no:  9\n","\n","question no:  10\n","Cheking Answer\n","Answer do not exist for Question no:  10\n","\n","question no:  11\n","Cheking Answer\n","Answer do not exist for Question no:  11\n","--------------------------------------------------------------------\n","\n","Pragraph no:  2\n","Paragraph no:  2  was found in arabic\n","Questions:\n","\n","question no:  0\n","Cheking Answer\n","Answer do not exist for Question no:  0\n","\n","question no:  1\n","Cheking Answer\n","Answer do not exist for Question no:  1\n","\n","question no:  2\n","Cheking Answer\n","Answer do not exist for Question no:  2\n","\n","question no:  3\n","Cheking Answer\n","Answer do not exist for Question no:  3\n","\n","question no:  4\n","Cheking Answer\n","Answer do not exist for Question no:  4\n","\n","question no:  5\n","Cheking Answer\n","Answer do not exist for Question no:  5\n","\n","question no:  6\n","Cheking Answer\n","Answer do not exist for Question no:  6\n","\n","question no:  7\n","Cheking Answer\n","Answer exists for Question no:  7\n","\n","question no:  8\n","Cheking Answer\n","Answer exists for Question no:  8\n","\n","question no:  9\n","Cheking Answer\n","Answer exists for Question no:  9\n","\n","question no:  10\n","Cheking Answer\n","Answer do not exist for Question no:  10\n","\n","question no:  11\n","Cheking Answer\n","Answer do not exist for Question no:  11\n","--------------------------------------------------------------------\n","\n","Pragraph no:  3\n","Paragraph no:  3  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  4\n","Paragraph no:  4  was found in arabic\n","Questions:\n","\n","question no:  0\n","Cheking Answer\n","Answer do not exist for Question no:  0\n","\n","question no:  1\n","Cheking Answer\n","Answer exists for Question no:  1\n","\n","question no:  2\n","Cheking Answer\n","Answer exists for Question no:  2\n","\n","question no:  3\n","Cheking Answer\n","Answer exists for Question no:  3\n","\n","question no:  4\n","Cheking Answer\n","Answer do not exist for Question no:  4\n","\n","question no:  5\n","Cheking Answer\n","Answer do not exist for Question no:  5\n","\n","question no:  6\n","Cheking Answer\n","Answer exists for Question no:  6\n","\n","question no:  7\n","Cheking Answer\n","Answer do not exist for Question no:  7\n","\n","question no:  8\n","Cheking Answer\n","Answer exists for Question no:  8\n","\n","question no:  9\n","Cheking Answer\n","Answer do not exist for Question no:  9\n","\n","question no:  10\n","Cheking Answer\n","Answer do not exist for Question no:  10\n","--------------------------------------------------------------------\n","\n","Pragraph no:  5\n","Paragraph no:  5  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  6\n","Paragraph no:  6  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  7\n","Paragraph no:  7  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  8\n","Paragraph no:  8  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  9\n","Paragraph no:  9  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  10\n","Paragraph no:  10  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  11\n","Paragraph no:  11  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  12\n","Paragraph no:  12  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  13\n","Paragraph no:  13  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  14\n","Paragraph no:  14  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  15\n","Paragraph no:  15  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  16\n","Paragraph no:  16  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  17\n","Paragraph no:  17  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  18\n","Paragraph no:  18  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  19\n","Paragraph no:  19  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  20\n","Paragraph no:  20  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  21\n","Paragraph no:  21  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  22\n","Paragraph no:  22  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  23\n","Paragraph no:  23  was found in arabic\n","Questions:\n","\n","question no:  0\n","Cheking Answer\n","Answer do not exist for Question no:  0\n","\n","question no:  1\n","Cheking Answer\n","Answer do not exist for Question no:  1\n","\n","question no:  2\n","Cheking Answer\n","Answer do not exist for Question no:  2\n","\n","question no:  3\n","Cheking Answer\n","Answer exists for Question no:  3\n","\n","question no:  4\n","Cheking Answer\n","Answer do not exist for Question no:  4\n","\n","question no:  5\n","Cheking Answer\n","Answer do not exist for Question no:  5\n","\n","question no:  6\n","Cheking Answer\n","Answer do not exist for Question no:  6\n","\n","question no:  7\n","Cheking Answer\n","Answer do not exist for Question no:  7\n","\n","question no:  8\n","Cheking Answer\n","Answer do not exist for Question no:  8\n","\n","question no:  9\n","Cheking Answer\n","Answer do not exist for Question no:  9\n","--------------------------------------------------------------------\n","\n","Pragraph no:  24\n","Paragraph no:  24  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  25\n","Paragraph no:  25  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  26\n","Paragraph no:  26  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  27\n","Paragraph no:  27  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  28\n","Paragraph no:  28  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  29\n","Paragraph no:  29  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  30\n","Paragraph no:  30  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  31\n","Paragraph no:  31  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  32\n","Paragraph no:  32  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  33\n","Paragraph no:  33  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  34\n","Paragraph no:  34  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  35\n","Paragraph no:  35  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  36\n","Paragraph no:  36  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  37\n","Paragraph no:  37  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  38\n","Paragraph no:  38  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  39\n","Paragraph no:  39  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  40\n","Paragraph no:  40  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  41\n","Paragraph no:  41  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  42\n","Paragraph no:  42  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  43\n","Paragraph no:  43  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  44\n","Paragraph no:  44  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  45\n","Paragraph no:  45  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  46\n","Paragraph no:  46  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  47\n","Paragraph no:  47  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  48\n","Paragraph no:  48  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  49\n","Paragraph no:  49  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  50\n","Paragraph no:  50  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  51\n","Paragraph no:  51  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  52\n","Paragraph no:  52  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  53\n","Paragraph no:  53  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  54\n","Paragraph no:  54  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  55\n","Paragraph no:  55  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  56\n","Paragraph no:  56  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  57\n","Paragraph no:  57  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  58\n","Paragraph no:  58  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  59\n","Paragraph no:  59  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  60\n","Paragraph no:  60  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  61\n","Paragraph no:  61  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  62\n","Paragraph no:  62  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  63\n","Paragraph no:  63  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  64\n","Paragraph no:  64  was NOT found in arabic\n","--------------------------------------------------------------------\n","\n","Pragraph no:  65\n","Paragraph no:  65  was NOT found in arabic\n","--------------------------------------------------------------------\n","##################################################\n","da5al count:  1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zlIe8BAPkzhj","colab_type":"code","outputId":"6f6e451b-4bab-446b-ea1d-2fec81261a74","executionInfo":{"status":"ok","timestamp":1566497661166,"user_tz":-120,"elapsed":1104,"user":{"displayName":"Adel Meleka","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDTHA8EkbdalWzWPNphz2U6EctjU2uloExj3Mr3_Q=s64","userId":"09805447388762865418"}},"colab":{"base_uri":"https://localhost:8080/","height":67}},"source":["# import json\n","\n","# qas = []\n","\n","# ans_details = {'text': '20 years old',\n","#                'answer_start':'50'}\n","\n","# qas.append({'question' : 'how old are you',\n","#             'id': '1',\n","#             'answers': [ans_details],\n","#             'is_impossible':'false'})\n","\n","# data = {}\n","# data['test'] = qas\n","# with open('./data/test.json', 'w') as outfile:\n","#     json.dump(data, outfile)\n","\n","\n","p = ['a','3','fd']\n","\n","for e in p:\n","  print (p.index(e))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0\n","1\n","2\n"],"name":"stdout"}]}]}