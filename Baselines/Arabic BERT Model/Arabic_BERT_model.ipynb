{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Arabic BERT model",
      "provenance": [],
      "collapsed_sections": [
        "GHE5JuBDl8PV",
        "nHmQiXG2QM7x",
        "V8e_FZwJPxOh",
        "3gm3cfyaO8gs",
        "ji-Dhq5pA2We"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqJ7wHgkjYRm",
        "colab_type": "text"
      },
      "source": [
        "# Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4foW_0zGARn",
        "colab_type": "code",
        "outputId": "63c13eba-c0e6-447a-f240-03035df94053",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77w-iKk8Qr35",
        "colab_type": "text"
      },
      "source": [
        "# ARGS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i58dsCZ6QumS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_file_path = \"/content/drive/My Drive/AAQAD/BERT Model/Pretrained/vocab.txt\"#bert vocab file\n",
        "bert_config_file_path = \"/content/drive/My Drive/AAQAD/BERT Model/Pretrained/bert_config.json\"#bert config file\n",
        "bert_init_checkpoint_training_path= \"/content/drive/My Drive/AAQAD/BERT Model/Pretrained/bert_model.ckpt\"#bert multilingual pretrained model\n",
        "output_dir = \"/content/drive/My Drive/AAQAD/BERT Model/Fine-tuned/\"#dir of training output\n",
        "predictions_file_path = \"/content/drive/My Drive/AAQAD/BERT Model/Fine-tuned/predictions.json\"#predictions output file\n",
        "AAQAD_train_set_path = \"/content/drive/My Drive/AAQAD/data/AAQAD-train.json\"#AAQAD training set\n",
        "AAQAD_dev_set_path = \"/content/drive/My Drive/AAQAD/data/AAQAD-dev.json\"#AAQAD dev set\n",
        "AAQAD_test_set_path = \"/content/drive/My Drive/AAQAD/data/AAQAD-test.json\"#AAQAD test set\n",
        "checkpoint_file_dir = \"/content/drive/My Drive/AAQAD/BERT Model/Fine-tuned/\"#model checkpoint directory\n",
        "output_dir_command = \"/content/drive/My\\\\ Drive/AAQAD/BERT\\\\ Model/Fine\\\\-tuned/\"#used for checkpoint set in command line"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHE5JuBDl8PV",
        "colab_type": "text"
      },
      "source": [
        "# UTILS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCZY4x-Cl-CY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "import json\n",
        "import textwrap\n",
        "import numpy as np\n",
        "import torch\n",
        "from math import floor\n",
        "import os\n",
        "import random\n",
        "import copy\n",
        "# Stanford functions\n",
        "def torch_from_json(path, dtype=torch.float32):\n",
        "    \"\"\"Load a PyTorch Tensor from a JSON file.\n",
        "\n",
        "    Args:\n",
        "        path (str): Path to the JSON file to load.\n",
        "        dtype (torch.dtype): Data type of loaded array.\n",
        "\n",
        "    Returns:\n",
        "        tensor (torch.Tensor): Tensor loaded from JSON file.\n",
        "    \"\"\"\n",
        "    with open(path, 'r') as fh:\n",
        "        array = np.array(json.load(fh))\n",
        "\n",
        "    tensor = torch.from_numpy(array).type(dtype)\n",
        "\n",
        "    return tensor\n",
        "# AQAAD functions\n",
        "def saveJSON(path, data):\n",
        "  jsonVar = json.dumps(data, ensure_ascii=False)\n",
        "  json_file = io.open(path, mode=\"w\", encoding=\"utf-8\")\n",
        "  json_file.write(jsonVar)\n",
        "  json_file.close()\n",
        "def loadJSON(path):\n",
        "  json_file = io.open(path, mode=\"r\", encoding=\"utf-8\")\n",
        "  return json.load(json_file)\n",
        "def printWrapped(text, lineWidth):\n",
        "  wrapper = textwrap.TextWrapper(width=lineWidth) \n",
        "  word_list = wrapper.wrap(text=text)  \n",
        "  for element in word_list: \n",
        "    print(element)\n",
        "def getFile(path, op):\n",
        "  return io.open(path, mode=op, encoding=\"utf-8\")\n",
        "def combineFilesIn(fNames, ofName, combinedAttr):\n",
        "  firstJSONFile = loadJSON(fNames[0])\n",
        "  attrDict = firstJSONFile[combinedAttr]\n",
        "  for i in range(1, len(fNames)):\n",
        "    attrDict = attrDict + loadJSON(fNames[i])[combinedAttr]\n",
        "  firstJSONFile[combinedAttr] = attrDict\n",
        "  saveJSON(ofName, firstJSONFile)\n",
        "def getEvalSummaryFromFile(logFile, outputFile):\n",
        "  f= open(logFile, \"r\")\n",
        "  lines = f.readlines()\n",
        "  of = open(outputFile, \"w\")\n",
        "  output = \"\"\n",
        "  j = 0\n",
        "  for i in lines:\n",
        "    if \"F1:\" in i:\n",
        "      output = output + \"evaluation #\" + str(j) + \": \" + i[i.find(\"Dev\"):]\n",
        "      j+=1\n",
        "  of.write(output)\n",
        "  of.close()\n",
        "  print(output)\n",
        "def getEvalSummaryFromString(stringSummary, outputFile):\n",
        "  lines = stringSummary.splitlines()\n",
        "  of = open(outputFile, \"w\")\n",
        "  output = \"\"\n",
        "  j = 0\n",
        "  for i in lines:\n",
        "    if \"F1:\" in i:\n",
        "      output = output + \"\\n\\r\" + \"evaluation #\" + str(j) + \": \" + i[i.find(\"Dev\"):]\n",
        "      j+=1\n",
        "  of.write(output)\n",
        "  of.close()\n",
        "  print(output)\n",
        "\n",
        "def fixEncoding(path):\n",
        "  json_file = loadJSON(path)\n",
        "  saveJSON(path, json_file)\n",
        "\n",
        "def convertToVersion2(path):\n",
        "  squadDict = loadJSON(path)\n",
        "  for item in squadDict[\"data\"]:\n",
        "    for parag in item[\"paragraphs\"]:\n",
        "      for ques in parag[\"qas\"]:\n",
        "        ques[\"is_impossible\"] = False\n",
        "  saveJSON(path, squadDict)\n",
        "def convertToVersion1(path):\n",
        "  squad2Dict = loadJSON(path)\n",
        "  for item in squad2Dict[\"data\"]:\n",
        "    for parag in item[\"paragraphs\"]:\n",
        "      parag[\"qas\"] = [ques for ques in parag[\"qas\"] if not ques[\"is_impossible\"]]\n",
        "  saveJSON(path, squad2Dict)\n",
        "def getNumberOfQuestions(dataset):\n",
        "  tot_ques = 0\n",
        "  imp_ques = 0\n",
        "  for item in dataset[\"data\"]:\n",
        "    for parag in item[\"paragraphs\"]:\n",
        "      tot_ques += len(parag[\"qas\"])\n",
        "      imp_ques += sum(\"is_impossible\" in ques and ques[\"is_impossible\"] for ques in parag[\"qas\"])\n",
        "  return tot_ques, imp_ques\n",
        "\n",
        "def randomSample(path, nQuestions, outputPath):\n",
        "  origDataset = loadJSON(path)\n",
        "  nOrigQ, _ = getNumberOfQuestions(origDataset)\n",
        "  nQuesToBeDeleted = nOrigQ - nQuestions\n",
        "  data = origDataset[\"data\"]\n",
        "  while nQuesToBeDeleted > 0:\n",
        "    randArticle = data[random.randint(0, len(data) - 1)]\n",
        "    parags = randArticle[\"paragraphs\"]\n",
        "    randParag = parags[random.randint(0, len(parags) - 1)]\n",
        "    questions = randParag[\"qas\"]\n",
        "    questions.pop(random.randint(0, len(questions) - 1))\n",
        "    nQuesToBeDeleted -= 1\n",
        "    if len(questions) == 0:\n",
        "      parags.remove(randParag)\n",
        "    if len(parags) == 0:\n",
        "      data.remove(randArticle)\n",
        "  saveJSON(outputPath, origDataset)\n",
        "\n",
        "def randomSampleV2(dataset, nQuestions, nIQuestions,outputPath):\n",
        "  newDataset = copy.deepcopy(dataset)\n",
        "  nOrigQ, nOrigIQ = getNumberOfQuestions(newDataset)\n",
        "  nQuesToBeDeleted = nOrigQ - nQuestions\n",
        "  nIQuesToBeDeleted = nOrigIQ - nIQuestions\n",
        "  data = newDataset[\"data\"]\n",
        "  while nQuesToBeDeleted > 0:\n",
        "    randArticle = data[random.randint(0, len(data) - 1)]\n",
        "    parags = randArticle[\"paragraphs\"]\n",
        "    randParag = parags[random.randint(0, len(parags) - 1)]\n",
        "    questions = randParag[\"qas\"]\n",
        "    if nIQuesToBeDeleted > 0:\n",
        "      randQuesIndex = random.randint(0, len(questions) - 1)\n",
        "      if questions[randQuesIndex][\"is_impossible\"]:\n",
        "        questions.pop(randQuesIndex)\n",
        "        nIQuesToBeDeleted -= 1\n",
        "        nQuesToBeDeleted -= 1\n",
        "    else:\n",
        "      randQuesIndex = random.randint(0, len(questions) - 1)\n",
        "      if not questions[randQuesIndex][\"is_impossible\"]:\n",
        "        questions.pop(randQuesIndex)\n",
        "        nQuesToBeDeleted -= 1\n",
        "    if len(questions) == 0:\n",
        "      parags.remove(randParag)\n",
        "    if len(parags) == 0:\n",
        "      data.remove(randArticle)\n",
        "  saveJSON(outputPath, newDataset)\n",
        "  return newDataset\n",
        "\n",
        "def removeSubDataset(subDataset, dataset):\n",
        "  newDataset = copy.deepcopy(dataset)\n",
        "  data = newDataset[\"data\"]\n",
        "  subData = subDataset[\"data\"]\n",
        "  j = 0\n",
        "  for i in range(len(data)):\n",
        "    item = data[i]\n",
        "    if item[\"title\"] == subData[j][\"title\"]:\n",
        "      parags = item[\"paragraphs\"]\n",
        "      subParags = subData[j][\"paragraphs\"]\n",
        "      l = 0\n",
        "      for k in range(len(parags)):\n",
        "        parag = parags[k]\n",
        "        if parag[\"context\"] == subParags[l][\"context\"]:\n",
        "          questions = parag[\"qas\"]\n",
        "          subQuestions = subParags[l][\"qas\"]\n",
        "          newQuestions = [i for i in questions if not item in subQuestions]\n",
        "          if len(newQuestions) == 0:\n",
        "            parags.remove(parag)\n",
        "            k -= 1\n",
        "          if len(parags) == 0:\n",
        "            data.remove(item)\n",
        "            i -= 1\n",
        "          l += 1\n",
        "          if l == len(subParags):\n",
        "            break\n",
        "      j += 1\n",
        "      if j == len(subData):\n",
        "        break\n",
        "  newDataset[\"data\"] = data\n",
        "  return newDataset\n",
        "  \n",
        "\n",
        "def getDatasetSummary(dataset):\n",
        "  tot_ques = 0\n",
        "  imp_ques = 0\n",
        "  tot_articles = len(dataset[\"data\"])\n",
        "  tot_parag = 0\n",
        "  for item in dataset[\"data\"]:\n",
        "    tot_parag += len(item[\"paragraphs\"])\n",
        "    for parag in item[\"paragraphs\"]:\n",
        "      tot_ques += len(parag[\"qas\"])\n",
        "      imp_ques += sum(\"is_impossible\" in ques and ques[\"is_impossible\"] for ques in parag[\"qas\"])\n",
        "  return tot_articles, tot_parag, tot_ques, imp_ques\n",
        "\n",
        "def splitDataset(dataset, trainRatio, devRatio, trainPath, devPath, testPath):\n",
        "  nQ, niQ = getNumberOfQuestions(dataset)\n",
        "  nTrainQ, nTrainIQ, nDevQ, nDevIQ = floor(nQ * trainRatio), floor(niQ * trainRatio), floor(nQ * devRatio), floor(niQ * devRatio)\n",
        "  nTestQ, nTestIQ = nQ - nTrainQ - nDevQ, niQ - nTrainIQ - nDevIQ\n",
        "  trainSet = randomSampleV2(dataset, nTrainQ, nTrainIQ, trainPath)\n",
        "  remainingDataset = removeSubDataset(trainSet, dataset)\n",
        "  devSet = randomSampleV2(remainingDataset, nDevQ, nDevIQ, devPath)\n",
        "  remainingDataset = removeSubDataset(devSet, remainingDataset)\n",
        "  testSet = randomSampleV2(remainingDataset, nTestQ, nTestIQ, testPath)\n",
        "  return trainSet, devSet, testSet\n",
        "  \n",
        "def printDatasetSummary(dataset, name):\n",
        "  nArt, nParag, nQ, niQ = getDatasetSummary(dataset)\n",
        "  print(\"summary of \" + name +\": \")\n",
        "  print(\"Number of articles: \" + str(nArt) + \", Number of Paragraphs: \" + str(nParag) + \", Number of questions: \" + str(nQ) + \", Number of impossible questions: \" + str(niQ), \", ratio of impossible question: \" + str(niQ/nQ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBedXiu249_y",
        "colab_type": "text"
      },
      "source": [
        "# Make directories and download necessary files - skip if you already have the data in the directories above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwsZkOjVk8Eu",
        "colab_type": "text"
      },
      "source": [
        "## download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmuTNpIK5G3u",
        "colab_type": "code",
        "outputId": "cada440c-700f-4f9f-f6a3-f83b44ab7f17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        }
      },
      "source": [
        "%mkdir /content/drive/\n",
        "%mkdir /content/drive/My\\ Drive/\n",
        "%mkdir /content/drive/My\\ Drive/AAQAD/\n",
        "%mkdir /content/drive/My\\ Drive/AAQAD/data/\n",
        "%cd /content/drive/My\\ Drive/AAQAD/data/\n",
        "!pip install gdown\n",
        "!gdown https://drive.google.com/uc?id=1jhUmWb9eHVATqhrWKAXxSE2gqJ53-wk6 -O AAQAD.json\n",
        "!gdown https://drive.google.com/uc?id=1V5ziIZe__pGg14nH42WyMEFz444XPWf7 -O AAQAD\\-train.json\n",
        "!gdown https://drive.google.com/uc?id=19nj9jiCdJlHwAfgUTJ_Z8jg1cB34yfjv -O AAQAD\\-dev.json\n",
        "!gdown https://drive.google.com/uc?id=1z0XksuTwnqhiX1guxkmjYmoNA_JZ6SUN -O AAQAD\\-test.json\n",
        "%mkdir /content/drive/My\\ Drive/AAQAD/BERT\\ Model/\n",
        "%mkdir /content/drive/My\\ Drive/AAQAD/BERT\\ Model/Pretrained/\n",
        "%cd /content/drive/My\\ Drive/AAQAD/BERT\\ Model/Pretrained/ \n",
        "!wget -cq https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip\n",
        "!unzip -qq -j multi_cased_L-12_H-768_A-12.zip\n",
        "!rm multi_cased_L-12_H-768_A-12.zip\n",
        "%mkdir /content/drive/My\\ Drive/AAQAD/BERT\\ Model/Fine\\-tuned/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/drive/’: File exists\n",
            "mkdir: cannot create directory ‘/content/drive/My Drive/’: File exists\n",
            "mkdir: cannot create directory ‘/content/drive/My Drive/AAQAD/’: File exists\n",
            "mkdir: cannot create directory ‘/content/drive/My Drive/AAQAD/data/’: File exists\n",
            "/content/drive/My Drive/AAQAD/data\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (3.6.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gdown) (4.28.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gdown) (1.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gdown) (2.21.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2019.9.11)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (3.0.4)\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1jhUmWb9eHVATqhrWKAXxSE2gqJ53-wk6\n",
            "To: /content/drive/My Drive/AAQAD/data/AAQAD.json\n",
            "9.94MB [00:00, 87.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1V5ziIZe__pGg14nH42WyMEFz444XPWf7\n",
            "To: /content/drive/My Drive/AAQAD/data/AAQAD-train.json\n",
            "5.96MB [00:00, 52.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=19nj9jiCdJlHwAfgUTJ_Z8jg1cB34yfjv\n",
            "To: /content/drive/My Drive/AAQAD/data/AAQAD-dev.json\n",
            "100% 822k/822k [00:00<00:00, 53.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1z0XksuTwnqhiX1guxkmjYmoNA_JZ6SUN\n",
            "To: /content/drive/My Drive/AAQAD/data/AAQAD-test.json\n",
            "100% 1.46M/1.46M [00:00<00:00, 92.3MB/s]\n",
            "mkdir: cannot create directory ‘/content/drive/My Drive/AAQAD/BERT Model/’: File exists\n",
            "mkdir: cannot create directory ‘/content/drive/My Drive/AAQAD/BERT Model/Pretrained/’: File exists\n",
            "/content/drive/My Drive/AAQAD/BERT Model/Pretrained\n",
            "replace bert_model.ckpt.meta? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHmQiXG2QM7x",
        "colab_type": "text"
      },
      "source": [
        "# tokenization.py ✔️"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kzba3U2QRGB",
        "colab_type": "code",
        "outputId": "c2c23c17-5337-48c3-f23e-357bd64e87d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655
        }
      },
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"Tokenization classes.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "import re\n",
        "import unicodedata\n",
        "import six\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def validate_case_matches_checkpoint(do_lower_case, init_checkpoint):\n",
        "  \"\"\"Checks whether the casing config is consistent with the checkpoint name.\"\"\"\n",
        "\n",
        "  # The casing has to be passed in by the user and there is no explicit check\n",
        "  # as to whether it matches the checkpoint. The casing information probably\n",
        "  # should have been stored in the bert_config.json file, but it's not, so\n",
        "  # we have to heuristically detect it to validate.\n",
        "\n",
        "  if not init_checkpoint:\n",
        "    return\n",
        "\n",
        "  m = re.match(\"^.*?([A-Za-z0-9_-]+)/bert_model.ckpt\", init_checkpoint)\n",
        "  if m is None:\n",
        "    return\n",
        "\n",
        "  model_name = m.group(1)\n",
        "\n",
        "  lower_models = [\n",
        "      \"uncased_L-24_H-1024_A-16\", \"uncased_L-12_H-768_A-12\",\n",
        "      \"multilingual_L-12_H-768_A-12\", \"chinese_L-12_H-768_A-12\"\n",
        "  ]\n",
        "\n",
        "  cased_models = [\n",
        "      \"cased_L-12_H-768_A-12\", \"cased_L-24_H-1024_A-16\",\n",
        "      \"multi_cased_L-12_H-768_A-12\"\n",
        "  ]\n",
        "\n",
        "  is_bad_config = False\n",
        "  if model_name in lower_models and not do_lower_case:\n",
        "    is_bad_config = True\n",
        "    actual_flag = \"False\"\n",
        "    case_name = \"lowercased\"\n",
        "    opposite_flag = \"True\"\n",
        "\n",
        "  if model_name in cased_models and do_lower_case:\n",
        "    is_bad_config = True\n",
        "    actual_flag = \"True\"\n",
        "    case_name = \"cased\"\n",
        "    opposite_flag = \"False\"\n",
        "\n",
        "  if is_bad_config:\n",
        "    raise ValueError(\n",
        "        \"You passed in `--do_lower_case=%s` with `--init_checkpoint=%s`. \"\n",
        "        \"However, `%s` seems to be a %s model, so you \"\n",
        "        \"should pass in `--do_lower_case=%s` so that the fine-tuning matches \"\n",
        "        \"how the model was pre-training. If this error is wrong, please \"\n",
        "        \"just comment out this check.\" % (actual_flag, init_checkpoint,\n",
        "                                          model_name, case_name, opposite_flag))\n",
        "\n",
        "\n",
        "def convert_to_unicode(text):\n",
        "  \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n",
        "  if six.PY3:\n",
        "    if isinstance(text, str):\n",
        "      return text\n",
        "    elif isinstance(text, bytes):\n",
        "      return text.decode(\"utf-8\", \"ignore\")\n",
        "    else:\n",
        "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "  elif six.PY2:\n",
        "    if isinstance(text, str):\n",
        "      return text.decode(\"utf-8\", \"ignore\")\n",
        "    elif isinstance(text, unicode):\n",
        "      return text\n",
        "    else:\n",
        "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "  else:\n",
        "    raise ValueError(\"Not running on Python2 or Python 3?\")\n",
        "\n",
        "\n",
        "def printable_text(text):\n",
        "  \"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"\n",
        "\n",
        "  # These functions want `str` for both Python2 and Python3, but in one case\n",
        "  # it's a Unicode string and in the other it's a byte string.\n",
        "  if six.PY3:\n",
        "    if isinstance(text, str):\n",
        "      return text\n",
        "    elif isinstance(text, bytes):\n",
        "      return text.decode(\"utf-8\", \"ignore\")\n",
        "    else:\n",
        "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "  elif six.PY2:\n",
        "    if isinstance(text, str):\n",
        "      return text\n",
        "    elif isinstance(text, unicode):\n",
        "      return text.encode(\"utf-8\")\n",
        "    else:\n",
        "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "  else:\n",
        "    raise ValueError(\"Not running on Python2 or Python 3?\")\n",
        "\n",
        "\n",
        "def load_vocab(vocab_file):\n",
        "  \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
        "  vocab = collections.OrderedDict()\n",
        "  index = 0\n",
        "  with tf.gfile.GFile(vocab_file, \"r\") as reader:\n",
        "    while True:\n",
        "      token = convert_to_unicode(reader.readline())\n",
        "      if not token:\n",
        "        break\n",
        "      token = token.strip()\n",
        "      vocab[token] = index\n",
        "      index += 1\n",
        "  return vocab\n",
        "\n",
        "\n",
        "def convert_by_vocab(vocab, items):\n",
        "  \"\"\"Converts a sequence of [tokens|ids] using the vocab.\"\"\"\n",
        "  output = []\n",
        "  for item in items:\n",
        "    output.append(vocab[item])\n",
        "  return output\n",
        "\n",
        "\n",
        "def convert_tokens_to_ids(vocab, tokens):\n",
        "  return convert_by_vocab(vocab, tokens)\n",
        "\n",
        "\n",
        "def convert_ids_to_tokens(inv_vocab, ids):\n",
        "  return convert_by_vocab(inv_vocab, ids)\n",
        "\n",
        "\n",
        "def whitespace_tokenize(text):\n",
        "  \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n",
        "  text = text.strip()\n",
        "  if not text:\n",
        "    return []\n",
        "  tokens = text.split()\n",
        "  return tokens\n",
        "\n",
        "\n",
        "class FullTokenizer(object):\n",
        "  \"\"\"Runs end-to-end tokenziation.\"\"\"\n",
        "\n",
        "  def __init__(self, vocab_file, do_lower_case=True):\n",
        "    self.vocab = load_vocab(vocab_file)\n",
        "    self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
        "    self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n",
        "    self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n",
        "\n",
        "  def tokenize(self, text):\n",
        "    split_tokens = []\n",
        "    for token in self.basic_tokenizer.tokenize(text):\n",
        "      for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
        "        split_tokens.append(sub_token)\n",
        "\n",
        "    return split_tokens\n",
        "\n",
        "  def convert_tokens_to_ids(self, tokens):\n",
        "    return convert_by_vocab(self.vocab, tokens)\n",
        "\n",
        "  def convert_ids_to_tokens(self, ids):\n",
        "    return convert_by_vocab(self.inv_vocab, ids)\n",
        "\n",
        "\n",
        "class BasicTokenizer(object):\n",
        "  \"\"\"Runs basic tokenization (punctuation splitting, lower casing, etc.).\"\"\"\n",
        "\n",
        "  def __init__(self, do_lower_case=True):\n",
        "    \"\"\"Constructs a BasicTokenizer.\n",
        "\n",
        "    Args:\n",
        "      do_lower_case: Whether to lower case the input.\n",
        "    \"\"\"\n",
        "    self.do_lower_case = do_lower_case\n",
        "\n",
        "  def tokenize(self, text):\n",
        "    \"\"\"Tokenizes a piece of text.\"\"\"\n",
        "    text = convert_to_unicode(text)\n",
        "    text = self._clean_text(text)\n",
        "\n",
        "    # This was added on November 1st, 2018 for the multilingual and Chinese\n",
        "    # models. This is also applied to the English models now, but it doesn't\n",
        "    # matter since the English models were not trained on any Chinese data\n",
        "    # and generally don't have any Chinese data in them (there are Chinese\n",
        "    # characters in the vocabulary because Wikipedia does have some Chinese\n",
        "    # words in the English Wikipedia.).\n",
        "    text = self._tokenize_chinese_chars(text)\n",
        "\n",
        "    orig_tokens = whitespace_tokenize(text)\n",
        "    split_tokens = []\n",
        "    for token in orig_tokens:\n",
        "      if self.do_lower_case:\n",
        "        token = token.lower()\n",
        "        token = self._run_strip_accents(token)\n",
        "      split_tokens.extend(self._run_split_on_punc(token))\n",
        "\n",
        "    output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n",
        "    return output_tokens\n",
        "\n",
        "  def _run_strip_accents(self, text):\n",
        "    \"\"\"Strips accents from a piece of text.\"\"\"\n",
        "    text = unicodedata.normalize(\"NFD\", text)\n",
        "    output = []\n",
        "    for char in text:\n",
        "      cat = unicodedata.category(char)\n",
        "      if cat == \"Mn\":\n",
        "        continue\n",
        "      output.append(char)\n",
        "    return \"\".join(output)\n",
        "\n",
        "  def _run_split_on_punc(self, text):\n",
        "    \"\"\"Splits punctuation on a piece of text.\"\"\"\n",
        "    chars = list(text)\n",
        "    i = 0\n",
        "    start_new_word = True\n",
        "    output = []\n",
        "    while i < len(chars):\n",
        "      char = chars[i]\n",
        "      if _is_punctuation(char):\n",
        "        output.append([char])\n",
        "        start_new_word = True\n",
        "      else:\n",
        "        if start_new_word:\n",
        "          output.append([])\n",
        "        start_new_word = False\n",
        "        output[-1].append(char)\n",
        "      i += 1\n",
        "\n",
        "    return [\"\".join(x) for x in output]\n",
        "\n",
        "  def _tokenize_chinese_chars(self, text):\n",
        "    \"\"\"Adds whitespace around any CJK character.\"\"\"\n",
        "    output = []\n",
        "    for char in text:\n",
        "      cp = ord(char)\n",
        "      if self._is_chinese_char(cp):\n",
        "        output.append(\" \")\n",
        "        output.append(char)\n",
        "        output.append(\" \")\n",
        "      else:\n",
        "        output.append(char)\n",
        "    return \"\".join(output)\n",
        "\n",
        "  def _is_chinese_char(self, cp):\n",
        "    \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n",
        "    # This defines a \"chinese character\" as anything in the CJK Unicode block:\n",
        "    #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n",
        "    #\n",
        "    # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n",
        "    # despite its name. The modern Korean Hangul alphabet is a different block,\n",
        "    # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n",
        "    # space-separated words, so they are not treated specially and handled\n",
        "    # like the all of the other languages.\n",
        "    if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n",
        "        (cp >= 0x3400 and cp <= 0x4DBF) or  #\n",
        "        (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n",
        "        (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n",
        "        (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n",
        "        (cp >= 0x2B820 and cp <= 0x2CEAF) or\n",
        "        (cp >= 0xF900 and cp <= 0xFAFF) or  #\n",
        "        (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n",
        "      return True\n",
        "\n",
        "    return False\n",
        "\n",
        "  def _clean_text(self, text):\n",
        "    \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n",
        "    output = []\n",
        "    for char in text:\n",
        "      cp = ord(char)\n",
        "      if cp == 0 or cp == 0xfffd or _is_control(char):\n",
        "        continue\n",
        "      if _is_whitespace(char):\n",
        "        output.append(\" \")\n",
        "      else:\n",
        "        output.append(char)\n",
        "    return \"\".join(output)\n",
        "\n",
        "\n",
        "class WordpieceTokenizer(object):\n",
        "  \"\"\"Runs WordPiece tokenziation.\"\"\"\n",
        "\n",
        "  def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=200):\n",
        "    self.vocab = vocab\n",
        "    self.unk_token = unk_token\n",
        "    self.max_input_chars_per_word = max_input_chars_per_word\n",
        "\n",
        "  def tokenize(self, text):\n",
        "    \"\"\"Tokenizes a piece of text into its word pieces.\n",
        "\n",
        "    This uses a greedy longest-match-first algorithm to perform tokenization\n",
        "    using the given vocabulary.\n",
        "\n",
        "    For example:\n",
        "      input = \"unaffable\"\n",
        "      output = [\"un\", \"##aff\", \"##able\"]\n",
        "\n",
        "    Args:\n",
        "      text: A single token or whitespace separated tokens. This should have\n",
        "        already been passed through `BasicTokenizer.\n",
        "\n",
        "    Returns:\n",
        "      A list of wordpiece tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    text = convert_to_unicode(text)\n",
        "\n",
        "    output_tokens = []\n",
        "    for token in whitespace_tokenize(text):\n",
        "      chars = list(token)\n",
        "      if len(chars) > self.max_input_chars_per_word:\n",
        "        output_tokens.append(self.unk_token)\n",
        "        continue\n",
        "\n",
        "      is_bad = False\n",
        "      start = 0\n",
        "      sub_tokens = []\n",
        "      while start < len(chars):\n",
        "        end = len(chars)\n",
        "        cur_substr = None\n",
        "        while start < end:\n",
        "          substr = \"\".join(chars[start:end])\n",
        "          if start > 0:\n",
        "            substr = \"##\" + substr\n",
        "          if substr in self.vocab:\n",
        "            cur_substr = substr\n",
        "            break\n",
        "          end -= 1\n",
        "        if cur_substr is None:\n",
        "          is_bad = True\n",
        "          break\n",
        "        sub_tokens.append(cur_substr)\n",
        "        start = end\n",
        "\n",
        "      if is_bad:\n",
        "        output_tokens.append(self.unk_token)\n",
        "      else:\n",
        "        output_tokens.extend(sub_tokens)\n",
        "    return output_tokens\n",
        "\n",
        "\n",
        "def _is_whitespace(char):\n",
        "  \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n",
        "  # \\t, \\n, and \\r are technically contorl characters but we treat them\n",
        "  # as whitespace since they are generally considered as such.\n",
        "  if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
        "    return True\n",
        "  cat = unicodedata.category(char)\n",
        "  if cat == \"Zs\":\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "\n",
        "def _is_control(char):\n",
        "  \"\"\"Checks whether `chars` is a control character.\"\"\"\n",
        "  # These are technically control characters but we count them as whitespace\n",
        "  # characters.\n",
        "  if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
        "    return False\n",
        "  cat = unicodedata.category(char)\n",
        "  if cat in (\"Cc\", \"Cf\"):\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "\n",
        "def _is_punctuation(char):\n",
        "  \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n",
        "  cp = ord(char)\n",
        "  # We treat all non-letter/number ASCII as punctuation.\n",
        "  # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n",
        "  # Punctuation class but we treat them as punctuation anyways, for\n",
        "  # consistency.\n",
        "  if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n",
        "      (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n",
        "    return True\n",
        "  cat = unicodedata.category(char)\n",
        "  if cat.startswith(\"P\"):\n",
        "    return True\n",
        "  return False\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8e_FZwJPxOh",
        "colab_type": "text"
      },
      "source": [
        "# optimization.py ✔️"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRtPcTkRP3Ac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"Functions and classes related to optimization (weight updates).\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import re\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def create_optimizer(loss, init_lr, num_train_steps, num_warmup_steps, use_tpu):\n",
        "  \"\"\"Creates an optimizer training op.\"\"\"\n",
        "  global_step = tf.train.get_or_create_global_step()\n",
        "\n",
        "  learning_rate = tf.constant(value=init_lr, shape=[], dtype=tf.float32)\n",
        "\n",
        "  # Implements linear decay of the learning rate.\n",
        "  learning_rate = tf.train.polynomial_decay(\n",
        "      learning_rate,\n",
        "      global_step,\n",
        "      num_train_steps,\n",
        "      end_learning_rate=0.0,\n",
        "      power=1.0,\n",
        "      cycle=False)\n",
        "\n",
        "  # Implements linear warmup. I.e., if global_step < num_warmup_steps, the\n",
        "  # learning rate will be `global_step/num_warmup_steps * init_lr`.\n",
        "  if num_warmup_steps:\n",
        "    global_steps_int = tf.cast(global_step, tf.int32)\n",
        "    warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.int32)\n",
        "\n",
        "    global_steps_float = tf.cast(global_steps_int, tf.float32)\n",
        "    warmup_steps_float = tf.cast(warmup_steps_int, tf.float32)\n",
        "\n",
        "    warmup_percent_done = global_steps_float / warmup_steps_float\n",
        "    warmup_learning_rate = init_lr * warmup_percent_done\n",
        "\n",
        "    is_warmup = tf.cast(global_steps_int < warmup_steps_int, tf.float32)\n",
        "    learning_rate = (\n",
        "        (1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)\n",
        "\n",
        "  # It is recommended that you use this optimizer for fine tuning, since this\n",
        "  # is how the model was trained (note that the Adam m/v variables are NOT\n",
        "  # loaded from init_checkpoint.)\n",
        "  optimizer = AdamWeightDecayOptimizer(\n",
        "      learning_rate=learning_rate,\n",
        "      weight_decay_rate=0.01,\n",
        "      beta_1=0.9,\n",
        "      beta_2=0.999,\n",
        "      epsilon=1e-6,\n",
        "      exclude_from_weight_decay=[\"LayerNorm\", \"layer_norm\", \"bias\"])\n",
        "\n",
        "  if use_tpu:\n",
        "    optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n",
        "\n",
        "  tvars = tf.trainable_variables()\n",
        "  grads = tf.gradients(loss, tvars)\n",
        "\n",
        "  # This is how the model was pre-trained.\n",
        "  (grads, _) = tf.clip_by_global_norm(grads, clip_norm=1.0)\n",
        "\n",
        "  train_op = optimizer.apply_gradients(\n",
        "      zip(grads, tvars), global_step=global_step)\n",
        "\n",
        "  # Normally the global step update is done inside of `apply_gradients`.\n",
        "  # However, `AdamWeightDecayOptimizer` doesn't do this. But if you use\n",
        "  # a different optimizer, you should probably take this line out.\n",
        "  new_global_step = global_step + 1\n",
        "  train_op = tf.group(train_op, [global_step.assign(new_global_step)])\n",
        "  return train_op\n",
        "\n",
        "\n",
        "class AdamWeightDecayOptimizer(tf.train.Optimizer):\n",
        "  \"\"\"A basic Adam optimizer that includes \"correct\" L2 weight decay.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               learning_rate,\n",
        "               weight_decay_rate=0.0,\n",
        "               beta_1=0.9,\n",
        "               beta_2=0.999,\n",
        "               epsilon=1e-6,\n",
        "               exclude_from_weight_decay=None,\n",
        "               name=\"AdamWeightDecayOptimizer\"):\n",
        "    \"\"\"Constructs a AdamWeightDecayOptimizer.\"\"\"\n",
        "    super(AdamWeightDecayOptimizer, self).__init__(False, name)\n",
        "\n",
        "    self.learning_rate = learning_rate\n",
        "    self.weight_decay_rate = weight_decay_rate\n",
        "    self.beta_1 = beta_1\n",
        "    self.beta_2 = beta_2\n",
        "    self.epsilon = epsilon\n",
        "    self.exclude_from_weight_decay = exclude_from_weight_decay\n",
        "\n",
        "  def apply_gradients(self, grads_and_vars, global_step=None, name=None):\n",
        "    \"\"\"See base class.\"\"\"\n",
        "    assignments = []\n",
        "    for (grad, param) in grads_and_vars:\n",
        "      if grad is None or param is None:\n",
        "        continue\n",
        "\n",
        "      param_name = self._get_variable_name(param.name)\n",
        "\n",
        "      m = tf.get_variable(\n",
        "          name=param_name + \"/adam_m\",\n",
        "          shape=param.shape.as_list(),\n",
        "          dtype=tf.float32,\n",
        "          trainable=False,\n",
        "          initializer=tf.zeros_initializer())\n",
        "      v = tf.get_variable(\n",
        "          name=param_name + \"/adam_v\",\n",
        "          shape=param.shape.as_list(),\n",
        "          dtype=tf.float32,\n",
        "          trainable=False,\n",
        "          initializer=tf.zeros_initializer())\n",
        "\n",
        "      # Standard Adam update.\n",
        "      next_m = (\n",
        "          tf.multiply(self.beta_1, m) + tf.multiply(1.0 - self.beta_1, grad))\n",
        "      next_v = (\n",
        "          tf.multiply(self.beta_2, v) + tf.multiply(1.0 - self.beta_2,\n",
        "                                                    tf.square(grad)))\n",
        "\n",
        "      update = next_m / (tf.sqrt(next_v) + self.epsilon)\n",
        "\n",
        "      # Just adding the square of the weights to the loss function is *not*\n",
        "      # the correct way of using L2 regularization/weight decay with Adam,\n",
        "      # since that will interact with the m and v parameters in strange ways.\n",
        "      #\n",
        "      # Instead we want ot decay the weights in a manner that doesn't interact\n",
        "      # with the m/v parameters. This is equivalent to adding the square\n",
        "      # of the weights to the loss with plain (non-momentum) SGD.\n",
        "      if self._do_use_weight_decay(param_name):\n",
        "        update += self.weight_decay_rate * param\n",
        "\n",
        "      update_with_lr = self.learning_rate * update\n",
        "\n",
        "      next_param = param - update_with_lr\n",
        "\n",
        "      assignments.extend(\n",
        "          [param.assign(next_param),\n",
        "           m.assign(next_m),\n",
        "           v.assign(next_v)])\n",
        "    return tf.group(*assignments, name=name)\n",
        "\n",
        "  def _do_use_weight_decay(self, param_name):\n",
        "    \"\"\"Whether to use L2 weight decay for `param_name`.\"\"\"\n",
        "    if not self.weight_decay_rate:\n",
        "      return False\n",
        "    if self.exclude_from_weight_decay:\n",
        "      for r in self.exclude_from_weight_decay:\n",
        "        if re.search(r, param_name) is not None:\n",
        "          return False\n",
        "    return True\n",
        "\n",
        "  def _get_variable_name(self, param_name):\n",
        "    \"\"\"Get the variable name from the tensor name.\"\"\"\n",
        "    m = re.match(\"^(.*):\\\\d+$\", param_name)\n",
        "    if m is not None:\n",
        "      param_name = m.group(1)\n",
        "    return param_name\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gm3cfyaO8gs",
        "colab_type": "text"
      },
      "source": [
        "# modeling.py-✔️"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfpcEpdiPDi5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"The main BERT model and related functions.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "import copy\n",
        "import json\n",
        "import math\n",
        "import re\n",
        "import numpy as np\n",
        "import six\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class BertConfig(object):\n",
        "  \"\"\"Configuration for `BertModel`.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               vocab_size,\n",
        "               hidden_size=768,\n",
        "               num_hidden_layers=12,\n",
        "               num_attention_heads=12,\n",
        "               intermediate_size=3072,\n",
        "               hidden_act=\"gelu\",\n",
        "               hidden_dropout_prob=0.1,\n",
        "               attention_probs_dropout_prob=0.1,\n",
        "               max_position_embeddings=512,\n",
        "               type_vocab_size=16,\n",
        "               initializer_range=0.02):\n",
        "    \"\"\"Constructs BertConfig.\n",
        "\n",
        "    Args:\n",
        "      vocab_size: Vocabulary size of `inputs_ids` in `BertModel`.\n",
        "      hidden_size: Size of the encoder layers and the pooler layer.\n",
        "      num_hidden_layers: Number of hidden layers in the Transformer encoder.\n",
        "      num_attention_heads: Number of attention heads for each attention layer in\n",
        "        the Transformer encoder.\n",
        "      intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n",
        "        layer in the Transformer encoder.\n",
        "      hidden_act: The non-linear activation function (function or string) in the\n",
        "        encoder and pooler.\n",
        "      hidden_dropout_prob: The dropout probability for all fully connected\n",
        "        layers in the embeddings, encoder, and pooler.\n",
        "      attention_probs_dropout_prob: The dropout ratio for the attention\n",
        "        probabilities.\n",
        "      max_position_embeddings: The maximum sequence length that this model might\n",
        "        ever be used with. Typically set this to something large just in case\n",
        "        (e.g., 512 or 1024 or 2048).\n",
        "      type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n",
        "        `BertModel`.\n",
        "      initializer_range: The stdev of the truncated_normal_initializer for\n",
        "        initializing all weight matrices.\n",
        "    \"\"\"\n",
        "    self.vocab_size = vocab_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_hidden_layers = num_hidden_layers\n",
        "    self.num_attention_heads = num_attention_heads\n",
        "    self.hidden_act = hidden_act\n",
        "    self.intermediate_size = intermediate_size\n",
        "    self.hidden_dropout_prob = hidden_dropout_prob\n",
        "    self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
        "    self.max_position_embeddings = max_position_embeddings\n",
        "    self.type_vocab_size = type_vocab_size\n",
        "    self.initializer_range = initializer_range\n",
        "\n",
        "  @classmethod\n",
        "  def from_dict(cls, json_object):\n",
        "    \"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"\n",
        "    config = BertConfig(vocab_size=None)\n",
        "    for (key, value) in six.iteritems(json_object):\n",
        "      config.__dict__[key] = value\n",
        "    return config\n",
        "\n",
        "  @classmethod\n",
        "  def from_json_file(cls, json_file):\n",
        "    \"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"\n",
        "    with tf.gfile.GFile(json_file, \"r\") as reader:\n",
        "      text = reader.read()\n",
        "    return cls.from_dict(json.loads(text))\n",
        "\n",
        "  def to_dict(self):\n",
        "    \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "    output = copy.deepcopy(self.__dict__)\n",
        "    return output\n",
        "\n",
        "  def to_json_string(self):\n",
        "    \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "    return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
        "\n",
        "\n",
        "class BertModel(object):\n",
        "  \"\"\"BERT model (\"Bidirectional Encoder Representations from Transformers\").\n",
        "\n",
        "  Example usage:\n",
        "\n",
        "  ```python\n",
        "  # Already been converted into WordPiece token ids\n",
        "  input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])\n",
        "  input_mask = tf.constant([[1, 1, 1], [1, 1, 0]])\n",
        "  token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])\n",
        "\n",
        "  config = modeling.BertConfig(vocab_size=32000, hidden_size=512,\n",
        "    num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n",
        "\n",
        "  model = modeling.BertModel(config=config, is_training=True,\n",
        "    input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids)\n",
        "\n",
        "  label_embeddings = tf.get_variable(...)\n",
        "  pooled_output = model.get_pooled_output()\n",
        "  logits = tf.matmul(pooled_output, label_embeddings)\n",
        "  ...\n",
        "  ```\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               config,\n",
        "               is_training,\n",
        "               input_ids,\n",
        "               input_mask=None,\n",
        "               token_type_ids=None,\n",
        "               use_one_hot_embeddings=False,\n",
        "               scope=None):\n",
        "    \"\"\"Constructor for BertModel.\n",
        "\n",
        "    Args:\n",
        "      config: `BertConfig` instance.\n",
        "      is_training: bool. true for training model, false for eval model. Controls\n",
        "        whether dropout will be applied.\n",
        "      input_ids: int32 Tensor of shape [batch_size, seq_length].\n",
        "      input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].\n",
        "      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n",
        "      use_one_hot_embeddings: (optional) bool. Whether to use one-hot word\n",
        "        embeddings or tf.embedding_lookup() for the word embeddings.\n",
        "      scope: (optional) variable scope. Defaults to \"bert\".\n",
        "\n",
        "    Raises:\n",
        "      ValueError: The config is invalid or one of the input tensor shapes\n",
        "        is invalid.\n",
        "    \"\"\"\n",
        "    config = copy.deepcopy(config)\n",
        "    if not is_training:\n",
        "      config.hidden_dropout_prob = 0.0\n",
        "      config.attention_probs_dropout_prob = 0.0\n",
        "\n",
        "    input_shape = get_shape_list(input_ids, expected_rank=2)\n",
        "    batch_size = input_shape[0]\n",
        "    seq_length = input_shape[1]\n",
        "\n",
        "    if input_mask is None:\n",
        "      input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)\n",
        "\n",
        "    if token_type_ids is None:\n",
        "      token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)\n",
        "\n",
        "    with tf.variable_scope(scope, default_name=\"bert\"):\n",
        "      with tf.variable_scope(\"embeddings\"):\n",
        "        # Perform embedding lookup on the word ids.\n",
        "        (self.embedding_output, self.embedding_table) = embedding_lookup(\n",
        "            input_ids=input_ids,\n",
        "            vocab_size=config.vocab_size,\n",
        "            embedding_size=config.hidden_size,\n",
        "            initializer_range=config.initializer_range,\n",
        "            word_embedding_name=\"word_embeddings\",\n",
        "            use_one_hot_embeddings=use_one_hot_embeddings)\n",
        "\n",
        "        # Add positional embeddings and token type embeddings, then layer\n",
        "        # normalize and perform dropout.\n",
        "        self.embedding_output = embedding_postprocessor(\n",
        "            input_tensor=self.embedding_output,\n",
        "            use_token_type=True,\n",
        "            token_type_ids=token_type_ids,\n",
        "            token_type_vocab_size=config.type_vocab_size,\n",
        "            token_type_embedding_name=\"token_type_embeddings\",\n",
        "            use_position_embeddings=True,\n",
        "            position_embedding_name=\"position_embeddings\",\n",
        "            initializer_range=config.initializer_range,\n",
        "            max_position_embeddings=config.max_position_embeddings,\n",
        "            dropout_prob=config.hidden_dropout_prob)\n",
        "\n",
        "      with tf.variable_scope(\"encoder\"):\n",
        "        # This converts a 2D mask of shape [batch_size, seq_length] to a 3D\n",
        "        # mask of shape [batch_size, seq_length, seq_length] which is used\n",
        "        # for the attention scores.\n",
        "        attention_mask = create_attention_mask_from_input_mask(\n",
        "            input_ids, input_mask)\n",
        "\n",
        "        # Run the stacked transformer.\n",
        "        # `sequence_output` shape = [batch_size, seq_length, hidden_size].\n",
        "        self.all_encoder_layers = transformer_model(\n",
        "            input_tensor=self.embedding_output,\n",
        "            attention_mask=attention_mask,\n",
        "            hidden_size=config.hidden_size,\n",
        "            num_hidden_layers=config.num_hidden_layers,\n",
        "            num_attention_heads=config.num_attention_heads,\n",
        "            intermediate_size=config.intermediate_size,\n",
        "            intermediate_act_fn=get_activation(config.hidden_act),\n",
        "            hidden_dropout_prob=config.hidden_dropout_prob,\n",
        "            attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n",
        "            initializer_range=config.initializer_range,\n",
        "            do_return_all_layers=True)\n",
        "\n",
        "      self.sequence_output = self.all_encoder_layers[-1]\n",
        "      # The \"pooler\" converts the encoded sequence tensor of shape\n",
        "      # [batch_size, seq_length, hidden_size] to a tensor of shape\n",
        "      # [batch_size, hidden_size]. This is necessary for segment-level\n",
        "      # (or segment-pair-level) classification tasks where we need a fixed\n",
        "      # dimensional representation of the segment.\n",
        "      with tf.variable_scope(\"pooler\"):\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "        # to the first token. We assume that this has been pre-trained\n",
        "        first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n",
        "        self.pooled_output = tf.layers.dense(\n",
        "            first_token_tensor,\n",
        "            config.hidden_size,\n",
        "            activation=tf.tanh,\n",
        "            kernel_initializer=create_initializer(config.initializer_range))\n",
        "\n",
        "  def get_pooled_output(self):\n",
        "    return self.pooled_output\n",
        "\n",
        "  def get_sequence_output(self):\n",
        "    \"\"\"Gets final hidden layer of encoder.\n",
        "\n",
        "    Returns:\n",
        "      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n",
        "      to the final hidden of the transformer encoder.\n",
        "    \"\"\"\n",
        "    return self.sequence_output\n",
        "\n",
        "  def get_all_encoder_layers(self):\n",
        "    return self.all_encoder_layers\n",
        "\n",
        "  def get_embedding_output(self):\n",
        "    \"\"\"Gets output of the embedding lookup (i.e., input to the transformer).\n",
        "\n",
        "    Returns:\n",
        "      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n",
        "      to the output of the embedding layer, after summing the word\n",
        "      embeddings with the positional embeddings and the token type embeddings,\n",
        "      then performing layer normalization. This is the input to the transformer.\n",
        "    \"\"\"\n",
        "    return self.embedding_output\n",
        "\n",
        "  def get_embedding_table(self):\n",
        "    return self.embedding_table\n",
        "\n",
        "\n",
        "def gelu(x):\n",
        "  \"\"\"Gaussian Error Linear Unit.\n",
        "\n",
        "  This is a smoother version of the RELU.\n",
        "  Original paper: https://arxiv.org/abs/1606.08415\n",
        "  Args:\n",
        "    x: float Tensor to perform activation.\n",
        "\n",
        "  Returns:\n",
        "    `x` with the GELU activation applied.\n",
        "  \"\"\"\n",
        "  cdf = 0.5 * (1.0 + tf.tanh(\n",
        "      (np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n",
        "  return x * cdf\n",
        "\n",
        "\n",
        "def get_activation(activation_string):\n",
        "  \"\"\"Maps a string to a Python function, e.g., \"relu\" => `tf.nn.relu`.\n",
        "\n",
        "  Args:\n",
        "    activation_string: String name of the activation function.\n",
        "\n",
        "  Returns:\n",
        "    A Python function corresponding to the activation function. If\n",
        "    `activation_string` is None, empty, or \"linear\", this will return None.\n",
        "    If `activation_string` is not a string, it will return `activation_string`.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: The `activation_string` does not correspond to a known\n",
        "      activation.\n",
        "  \"\"\"\n",
        "\n",
        "  # We assume that anything that\"s not a string is already an activation\n",
        "  # function, so we just return it.\n",
        "  if not isinstance(activation_string, six.string_types):\n",
        "    return activation_string\n",
        "\n",
        "  if not activation_string:\n",
        "    return None\n",
        "\n",
        "  act = activation_string.lower()\n",
        "  if act == \"linear\":\n",
        "    return None\n",
        "  elif act == \"relu\":\n",
        "    return tf.nn.relu\n",
        "  elif act == \"gelu\":\n",
        "    return gelu\n",
        "  elif act == \"tanh\":\n",
        "    return tf.tanh\n",
        "  else:\n",
        "    raise ValueError(\"Unsupported activation: %s\" % act)\n",
        "\n",
        "\n",
        "def get_assignment_map_from_checkpoint(tvars, init_checkpoint):\n",
        "  \"\"\"Compute the union of the current variables and checkpoint variables.\"\"\"\n",
        "  assignment_map = {}\n",
        "  initialized_variable_names = {}\n",
        "\n",
        "  name_to_variable = collections.OrderedDict()\n",
        "  for var in tvars:\n",
        "    name = var.name\n",
        "    m = re.match(\"^(.*):\\\\d+$\", name)\n",
        "    if m is not None:\n",
        "      name = m.group(1)\n",
        "    name_to_variable[name] = var\n",
        "\n",
        "  init_vars = tf.train.list_variables(init_checkpoint)\n",
        "\n",
        "  assignment_map = collections.OrderedDict()\n",
        "  for x in init_vars:\n",
        "    (name, var) = (x[0], x[1])\n",
        "    if name not in name_to_variable:\n",
        "      continue\n",
        "    assignment_map[name] = name\n",
        "    initialized_variable_names[name] = 1\n",
        "    initialized_variable_names[name + \":0\"] = 1\n",
        "\n",
        "  return (assignment_map, initialized_variable_names)\n",
        "\n",
        "\n",
        "def dropout(input_tensor, dropout_prob):\n",
        "  \"\"\"Perform dropout.\n",
        "\n",
        "  Args:\n",
        "    input_tensor: float Tensor.\n",
        "    dropout_prob: Python float. The probability of dropping out a value (NOT of\n",
        "      *keeping* a dimension as in `tf.nn.dropout`).\n",
        "\n",
        "  Returns:\n",
        "    A version of `input_tensor` with dropout applied.\n",
        "  \"\"\"\n",
        "  if dropout_prob is None or dropout_prob == 0.0:\n",
        "    return input_tensor\n",
        "\n",
        "  output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n",
        "  return output\n",
        "\n",
        "\n",
        "def layer_norm(input_tensor, name=None):\n",
        "  \"\"\"Run layer normalization on the last dimension of the tensor.\"\"\"\n",
        "  return tf.contrib.layers.layer_norm(\n",
        "      inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=name)\n",
        "\n",
        "\n",
        "def layer_norm_and_dropout(input_tensor, dropout_prob, name=None):\n",
        "  \"\"\"Runs layer normalization followed by dropout.\"\"\"\n",
        "  output_tensor = layer_norm(input_tensor, name)\n",
        "  output_tensor = dropout(output_tensor, dropout_prob)\n",
        "  return output_tensor\n",
        "\n",
        "\n",
        "def create_initializer(initializer_range=0.02):\n",
        "  \"\"\"Creates a `truncated_normal_initializer` with the given range.\"\"\"\n",
        "  return tf.truncated_normal_initializer(stddev=initializer_range)\n",
        "\n",
        "\n",
        "def embedding_lookup(input_ids,\n",
        "                     vocab_size,\n",
        "                     embedding_size=128,\n",
        "                     initializer_range=0.02,\n",
        "                     word_embedding_name=\"word_embeddings\",\n",
        "                     use_one_hot_embeddings=False):\n",
        "  \"\"\"Looks up words embeddings for id tensor.\n",
        "\n",
        "  Args:\n",
        "    input_ids: int32 Tensor of shape [batch_size, seq_length] containing word\n",
        "      ids.\n",
        "    vocab_size: int. Size of the embedding vocabulary.\n",
        "    embedding_size: int. Width of the word embeddings.\n",
        "    initializer_range: float. Embedding initialization range.\n",
        "    word_embedding_name: string. Name of the embedding table.\n",
        "    use_one_hot_embeddings: bool. If True, use one-hot method for word\n",
        "      embeddings. If False, use `tf.gather()`.\n",
        "\n",
        "  Returns:\n",
        "    float Tensor of shape [batch_size, seq_length, embedding_size].\n",
        "  \"\"\"\n",
        "  # This function assumes that the input is of shape [batch_size, seq_length,\n",
        "  # num_inputs].\n",
        "  #\n",
        "  # If the input is a 2D tensor of shape [batch_size, seq_length], we\n",
        "  # reshape to [batch_size, seq_length, 1].\n",
        "  if input_ids.shape.ndims == 2:\n",
        "    input_ids = tf.expand_dims(input_ids, axis=[-1])\n",
        "\n",
        "  embedding_table = tf.get_variable(\n",
        "      name=word_embedding_name,\n",
        "      shape=[vocab_size, embedding_size],\n",
        "      initializer=create_initializer(initializer_range))\n",
        "\n",
        "  flat_input_ids = tf.reshape(input_ids, [-1])\n",
        "  if use_one_hot_embeddings:\n",
        "    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)\n",
        "    output = tf.matmul(one_hot_input_ids, embedding_table)\n",
        "  else:\n",
        "    output = tf.gather(embedding_table, flat_input_ids)\n",
        "\n",
        "  input_shape = get_shape_list(input_ids)\n",
        "\n",
        "  output = tf.reshape(output,\n",
        "                      input_shape[0:-1] + [input_shape[-1] * embedding_size])\n",
        "  return (output, embedding_table)\n",
        "\n",
        "\n",
        "def embedding_postprocessor(input_tensor,\n",
        "                            use_token_type=False,\n",
        "                            token_type_ids=None,\n",
        "                            token_type_vocab_size=16,\n",
        "                            token_type_embedding_name=\"token_type_embeddings\",\n",
        "                            use_position_embeddings=True,\n",
        "                            position_embedding_name=\"position_embeddings\",\n",
        "                            initializer_range=0.02,\n",
        "                            max_position_embeddings=512,\n",
        "                            dropout_prob=0.1):\n",
        "  \"\"\"Performs various post-processing on a word embedding tensor.\n",
        "\n",
        "  Args:\n",
        "    input_tensor: float Tensor of shape [batch_size, seq_length,\n",
        "      embedding_size].\n",
        "    use_token_type: bool. Whether to add embeddings for `token_type_ids`.\n",
        "    token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n",
        "      Must be specified if `use_token_type` is True.\n",
        "    token_type_vocab_size: int. The vocabulary size of `token_type_ids`.\n",
        "    token_type_embedding_name: string. The name of the embedding table variable\n",
        "      for token type ids.\n",
        "    use_position_embeddings: bool. Whether to add position embeddings for the\n",
        "      position of each token in the sequence.\n",
        "    position_embedding_name: string. The name of the embedding table variable\n",
        "      for positional embeddings.\n",
        "    initializer_range: float. Range of the weight initialization.\n",
        "    max_position_embeddings: int. Maximum sequence length that might ever be\n",
        "      used with this model. This can be longer than the sequence length of\n",
        "      input_tensor, but cannot be shorter.\n",
        "    dropout_prob: float. Dropout probability applied to the final output tensor.\n",
        "\n",
        "  Returns:\n",
        "    float tensor with same shape as `input_tensor`.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: One of the tensor shapes or input values is invalid.\n",
        "  \"\"\"\n",
        "  input_shape = get_shape_list(input_tensor, expected_rank=3)\n",
        "  batch_size = input_shape[0]\n",
        "  seq_length = input_shape[1]\n",
        "  width = input_shape[2]\n",
        "\n",
        "  output = input_tensor\n",
        "\n",
        "  if use_token_type:\n",
        "    if token_type_ids is None:\n",
        "      raise ValueError(\"`token_type_ids` must be specified if\"\n",
        "                       \"`use_token_type` is True.\")\n",
        "    token_type_table = tf.get_variable(\n",
        "        name=token_type_embedding_name,\n",
        "        shape=[token_type_vocab_size, width],\n",
        "        initializer=create_initializer(initializer_range))\n",
        "    # This vocab will be small so we always do one-hot here, since it is always\n",
        "    # faster for a small vocabulary.\n",
        "    flat_token_type_ids = tf.reshape(token_type_ids, [-1])\n",
        "    one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size)\n",
        "    token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)\n",
        "    token_type_embeddings = tf.reshape(token_type_embeddings,\n",
        "                                       [batch_size, seq_length, width])\n",
        "    output += token_type_embeddings\n",
        "\n",
        "  if use_position_embeddings:\n",
        "    assert_op = tf.assert_less_equal(seq_length, max_position_embeddings)\n",
        "    with tf.control_dependencies([assert_op]):\n",
        "      full_position_embeddings = tf.get_variable(\n",
        "          name=position_embedding_name,\n",
        "          shape=[max_position_embeddings, width],\n",
        "          initializer=create_initializer(initializer_range))\n",
        "      # Since the position embedding table is a learned variable, we create it\n",
        "      # using a (long) sequence length `max_position_embeddings`. The actual\n",
        "      # sequence length might be shorter than this, for faster training of\n",
        "      # tasks that do not have long sequences.\n",
        "      #\n",
        "      # So `full_position_embeddings` is effectively an embedding table\n",
        "      # for position [0, 1, 2, ..., max_position_embeddings-1], and the current\n",
        "      # sequence has positions [0, 1, 2, ... seq_length-1], so we can just\n",
        "      # perform a slice.\n",
        "      position_embeddings = tf.slice(full_position_embeddings, [0, 0],\n",
        "                                     [seq_length, -1])\n",
        "      num_dims = len(output.shape.as_list())\n",
        "\n",
        "      # Only the last two dimensions are relevant (`seq_length` and `width`), so\n",
        "      # we broadcast among the first dimensions, which is typically just\n",
        "      # the batch size.\n",
        "      position_broadcast_shape = []\n",
        "      for _ in range(num_dims - 2):\n",
        "        position_broadcast_shape.append(1)\n",
        "      position_broadcast_shape.extend([seq_length, width])\n",
        "      position_embeddings = tf.reshape(position_embeddings,\n",
        "                                       position_broadcast_shape)\n",
        "      output += position_embeddings\n",
        "\n",
        "  output = layer_norm_and_dropout(output, dropout_prob)\n",
        "  return output\n",
        "\n",
        "\n",
        "def create_attention_mask_from_input_mask(from_tensor, to_mask):\n",
        "  \"\"\"Create 3D attention mask from a 2D tensor mask.\n",
        "\n",
        "  Args:\n",
        "    from_tensor: 2D or 3D Tensor of shape [batch_size, from_seq_length, ...].\n",
        "    to_mask: int32 Tensor of shape [batch_size, to_seq_length].\n",
        "\n",
        "  Returns:\n",
        "    float Tensor of shape [batch_size, from_seq_length, to_seq_length].\n",
        "  \"\"\"\n",
        "  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n",
        "  batch_size = from_shape[0]\n",
        "  from_seq_length = from_shape[1]\n",
        "\n",
        "  to_shape = get_shape_list(to_mask, expected_rank=2)\n",
        "  to_seq_length = to_shape[1]\n",
        "\n",
        "  to_mask = tf.cast(\n",
        "      tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n",
        "\n",
        "  # We don't assume that `from_tensor` is a mask (although it could be). We\n",
        "  # don't actually care if we attend *from* padding tokens (only *to* padding)\n",
        "  # tokens so we create a tensor of all ones.\n",
        "  #\n",
        "  # `broadcast_ones` = [batch_size, from_seq_length, 1]\n",
        "  broadcast_ones = tf.ones(\n",
        "      shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n",
        "\n",
        "  # Here we broadcast along two dimensions to create the mask.\n",
        "  mask = broadcast_ones * to_mask\n",
        "\n",
        "  return mask\n",
        "\n",
        "\n",
        "def attention_layer(from_tensor,\n",
        "                    to_tensor,\n",
        "                    attention_mask=None,\n",
        "                    num_attention_heads=1,\n",
        "                    size_per_head=512,\n",
        "                    query_act=None,\n",
        "                    key_act=None,\n",
        "                    value_act=None,\n",
        "                    attention_probs_dropout_prob=0.0,\n",
        "                    initializer_range=0.02,\n",
        "                    do_return_2d_tensor=False,\n",
        "                    batch_size=None,\n",
        "                    from_seq_length=None,\n",
        "                    to_seq_length=None):\n",
        "  \"\"\"Performs multi-headed attention from `from_tensor` to `to_tensor`.\n",
        "\n",
        "  This is an implementation of multi-headed attention based on \"Attention\n",
        "  is all you Need\". If `from_tensor` and `to_tensor` are the same, then\n",
        "  this is self-attention. Each timestep in `from_tensor` attends to the\n",
        "  corresponding sequence in `to_tensor`, and returns a fixed-with vector.\n",
        "\n",
        "  This function first projects `from_tensor` into a \"query\" tensor and\n",
        "  `to_tensor` into \"key\" and \"value\" tensors. These are (effectively) a list\n",
        "  of tensors of length `num_attention_heads`, where each tensor is of shape\n",
        "  [batch_size, seq_length, size_per_head].\n",
        "\n",
        "  Then, the query and key tensors are dot-producted and scaled. These are\n",
        "  softmaxed to obtain attention probabilities. The value tensors are then\n",
        "  interpolated by these probabilities, then concatenated back to a single\n",
        "  tensor and returned.\n",
        "\n",
        "  In practice, the multi-headed attention are done with transposes and\n",
        "  reshapes rather than actual separate tensors.\n",
        "\n",
        "  Args:\n",
        "    from_tensor: float Tensor of shape [batch_size, from_seq_length,\n",
        "      from_width].\n",
        "    to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width].\n",
        "    attention_mask: (optional) int32 Tensor of shape [batch_size,\n",
        "      from_seq_length, to_seq_length]. The values should be 1 or 0. The\n",
        "      attention scores will effectively be set to -infinity for any positions in\n",
        "      the mask that are 0, and will be unchanged for positions that are 1.\n",
        "    num_attention_heads: int. Number of attention heads.\n",
        "    size_per_head: int. Size of each attention head.\n",
        "    query_act: (optional) Activation function for the query transform.\n",
        "    key_act: (optional) Activation function for the key transform.\n",
        "    value_act: (optional) Activation function for the value transform.\n",
        "    attention_probs_dropout_prob: (optional) float. Dropout probability of the\n",
        "      attention probabilities.\n",
        "    initializer_range: float. Range of the weight initializer.\n",
        "    do_return_2d_tensor: bool. If True, the output will be of shape [batch_size\n",
        "      * from_seq_length, num_attention_heads * size_per_head]. If False, the\n",
        "      output will be of shape [batch_size, from_seq_length, num_attention_heads\n",
        "      * size_per_head].\n",
        "    batch_size: (Optional) int. If the input is 2D, this might be the batch size\n",
        "      of the 3D version of the `from_tensor` and `to_tensor`.\n",
        "    from_seq_length: (Optional) If the input is 2D, this might be the seq length\n",
        "      of the 3D version of the `from_tensor`.\n",
        "    to_seq_length: (Optional) If the input is 2D, this might be the seq length\n",
        "      of the 3D version of the `to_tensor`.\n",
        "\n",
        "  Returns:\n",
        "    float Tensor of shape [batch_size, from_seq_length,\n",
        "      num_attention_heads * size_per_head]. (If `do_return_2d_tensor` is\n",
        "      true, this will be of shape [batch_size * from_seq_length,\n",
        "      num_attention_heads * size_per_head]).\n",
        "\n",
        "  Raises:\n",
        "    ValueError: Any of the arguments or tensor shapes are invalid.\n",
        "  \"\"\"\n",
        "\n",
        "  def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n",
        "                           seq_length, width):\n",
        "    output_tensor = tf.reshape(\n",
        "        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n",
        "\n",
        "    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n",
        "    return output_tensor\n",
        "\n",
        "  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n",
        "  to_shape = get_shape_list(to_tensor, expected_rank=[2, 3])\n",
        "\n",
        "  if len(from_shape) != len(to_shape):\n",
        "    raise ValueError(\n",
        "        \"The rank of `from_tensor` must match the rank of `to_tensor`.\")\n",
        "\n",
        "  if len(from_shape) == 3:\n",
        "    batch_size = from_shape[0]\n",
        "    from_seq_length = from_shape[1]\n",
        "    to_seq_length = to_shape[1]\n",
        "  elif len(from_shape) == 2:\n",
        "    if (batch_size is None or from_seq_length is None or to_seq_length is None):\n",
        "      raise ValueError(\n",
        "          \"When passing in rank 2 tensors to attention_layer, the values \"\n",
        "          \"for `batch_size`, `from_seq_length`, and `to_seq_length` \"\n",
        "          \"must all be specified.\")\n",
        "\n",
        "  # Scalar dimensions referenced here:\n",
        "  #   B = batch size (number of sequences)\n",
        "  #   F = `from_tensor` sequence length\n",
        "  #   T = `to_tensor` sequence length\n",
        "  #   N = `num_attention_heads`\n",
        "  #   H = `size_per_head`\n",
        "\n",
        "  from_tensor_2d = reshape_to_matrix(from_tensor)\n",
        "  to_tensor_2d = reshape_to_matrix(to_tensor)\n",
        "\n",
        "  # `query_layer` = [B*F, N*H]\n",
        "  query_layer = tf.layers.dense(\n",
        "      from_tensor_2d,\n",
        "      num_attention_heads * size_per_head,\n",
        "      activation=query_act,\n",
        "      name=\"query\",\n",
        "      kernel_initializer=create_initializer(initializer_range))\n",
        "\n",
        "  # `key_layer` = [B*T, N*H]\n",
        "  key_layer = tf.layers.dense(\n",
        "      to_tensor_2d,\n",
        "      num_attention_heads * size_per_head,\n",
        "      activation=key_act,\n",
        "      name=\"key\",\n",
        "      kernel_initializer=create_initializer(initializer_range))\n",
        "\n",
        "  # `value_layer` = [B*T, N*H]\n",
        "  value_layer = tf.layers.dense(\n",
        "      to_tensor_2d,\n",
        "      num_attention_heads * size_per_head,\n",
        "      activation=value_act,\n",
        "      name=\"value\",\n",
        "      kernel_initializer=create_initializer(initializer_range))\n",
        "\n",
        "  # `query_layer` = [B, N, F, H]\n",
        "  query_layer = transpose_for_scores(query_layer, batch_size,\n",
        "                                     num_attention_heads, from_seq_length,\n",
        "                                     size_per_head)\n",
        "\n",
        "  # `key_layer` = [B, N, T, H]\n",
        "  key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads,\n",
        "                                   to_seq_length, size_per_head)\n",
        "\n",
        "  # Take the dot product between \"query\" and \"key\" to get the raw\n",
        "  # attention scores.\n",
        "  # `attention_scores` = [B, N, F, T]\n",
        "  attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\n",
        "  attention_scores = tf.multiply(attention_scores,\n",
        "                                 1.0 / math.sqrt(float(size_per_head)))\n",
        "\n",
        "  if attention_mask is not None:\n",
        "    # `attention_mask` = [B, 1, F, T]\n",
        "    attention_mask = tf.expand_dims(attention_mask, axis=[1])\n",
        "\n",
        "    # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
        "    # masked positions, this operation will create a tensor which is 0.0 for\n",
        "    # positions we want to attend and -10000.0 for masked positions.\n",
        "    adder = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0\n",
        "\n",
        "    # Since we are adding it to the raw scores before the softmax, this is\n",
        "    # effectively the same as removing these entirely.\n",
        "    attention_scores += adder\n",
        "\n",
        "  # Normalize the attention scores to probabilities.\n",
        "  # `attention_probs` = [B, N, F, T]\n",
        "  attention_probs = tf.nn.softmax(attention_scores)\n",
        "\n",
        "  # This is actually dropping out entire tokens to attend to, which might\n",
        "  # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "  attention_probs = dropout(attention_probs, attention_probs_dropout_prob)\n",
        "\n",
        "  # `value_layer` = [B, T, N, H]\n",
        "  value_layer = tf.reshape(\n",
        "      value_layer,\n",
        "      [batch_size, to_seq_length, num_attention_heads, size_per_head])\n",
        "\n",
        "  # `value_layer` = [B, N, T, H]\n",
        "  value_layer = tf.transpose(value_layer, [0, 2, 1, 3])\n",
        "\n",
        "  # `context_layer` = [B, N, F, H]\n",
        "  context_layer = tf.matmul(attention_probs, value_layer)\n",
        "\n",
        "  # `context_layer` = [B, F, N, H]\n",
        "  context_layer = tf.transpose(context_layer, [0, 2, 1, 3])\n",
        "\n",
        "  if do_return_2d_tensor:\n",
        "    # `context_layer` = [B*F, N*H]\n",
        "    context_layer = tf.reshape(\n",
        "        context_layer,\n",
        "        [batch_size * from_seq_length, num_attention_heads * size_per_head])\n",
        "  else:\n",
        "    # `context_layer` = [B, F, N*H]\n",
        "    context_layer = tf.reshape(\n",
        "        context_layer,\n",
        "        [batch_size, from_seq_length, num_attention_heads * size_per_head])\n",
        "\n",
        "  return context_layer\n",
        "\n",
        "\n",
        "def transformer_model(input_tensor,\n",
        "                      attention_mask=None,\n",
        "                      hidden_size=768,\n",
        "                      num_hidden_layers=12,\n",
        "                      num_attention_heads=12,\n",
        "                      intermediate_size=3072,\n",
        "                      intermediate_act_fn=gelu,\n",
        "                      hidden_dropout_prob=0.1,\n",
        "                      attention_probs_dropout_prob=0.1,\n",
        "                      initializer_range=0.02,\n",
        "                      do_return_all_layers=False):\n",
        "  \"\"\"Multi-headed, multi-layer Transformer from \"Attention is All You Need\".\n",
        "\n",
        "  This is almost an exact implementation of the original Transformer encoder.\n",
        "\n",
        "  See the original paper:\n",
        "  https://arxiv.org/abs/1706.03762\n",
        "\n",
        "  Also see:\n",
        "  https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py\n",
        "\n",
        "  Args:\n",
        "    input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].\n",
        "    attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,\n",
        "      seq_length], with 1 for positions that can be attended to and 0 in\n",
        "      positions that should not be.\n",
        "    hidden_size: int. Hidden size of the Transformer.\n",
        "    num_hidden_layers: int. Number of layers (blocks) in the Transformer.\n",
        "    num_attention_heads: int. Number of attention heads in the Transformer.\n",
        "    intermediate_size: int. The size of the \"intermediate\" (a.k.a., feed\n",
        "      forward) layer.\n",
        "    intermediate_act_fn: function. The non-linear activation function to apply\n",
        "      to the output of the intermediate/feed-forward layer.\n",
        "    hidden_dropout_prob: float. Dropout probability for the hidden layers.\n",
        "    attention_probs_dropout_prob: float. Dropout probability of the attention\n",
        "      probabilities.\n",
        "    initializer_range: float. Range of the initializer (stddev of truncated\n",
        "      normal).\n",
        "    do_return_all_layers: Whether to also return all layers or just the final\n",
        "      layer.\n",
        "\n",
        "  Returns:\n",
        "    float Tensor of shape [batch_size, seq_length, hidden_size], the final\n",
        "    hidden layer of the Transformer.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: A Tensor shape or parameter is invalid.\n",
        "  \"\"\"\n",
        "  if hidden_size % num_attention_heads != 0:\n",
        "    raise ValueError(\n",
        "        \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "        \"heads (%d)\" % (hidden_size, num_attention_heads))\n",
        "\n",
        "  attention_head_size = int(hidden_size / num_attention_heads)\n",
        "  input_shape = get_shape_list(input_tensor, expected_rank=3)\n",
        "  batch_size = input_shape[0]\n",
        "  seq_length = input_shape[1]\n",
        "  input_width = input_shape[2]\n",
        "\n",
        "  # The Transformer performs sum residuals on all layers so the input needs\n",
        "  # to be the same as the hidden size.\n",
        "  if input_width != hidden_size:\n",
        "    raise ValueError(\"The width of the input tensor (%d) != hidden size (%d)\" %\n",
        "                     (input_width, hidden_size))\n",
        "\n",
        "  # We keep the representation as a 2D tensor to avoid re-shaping it back and\n",
        "  # forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on\n",
        "  # the GPU/CPU but may not be free on the TPU, so we want to minimize them to\n",
        "  # help the optimizer.\n",
        "  prev_output = reshape_to_matrix(input_tensor)\n",
        "\n",
        "  all_layer_outputs = []\n",
        "  for layer_idx in range(num_hidden_layers):\n",
        "    with tf.variable_scope(\"layer_%d\" % layer_idx):\n",
        "      layer_input = prev_output\n",
        "\n",
        "      with tf.variable_scope(\"attention\"):\n",
        "        attention_heads = []\n",
        "        with tf.variable_scope(\"self\"):\n",
        "          attention_head = attention_layer(\n",
        "              from_tensor=layer_input,\n",
        "              to_tensor=layer_input,\n",
        "              attention_mask=attention_mask,\n",
        "              num_attention_heads=num_attention_heads,\n",
        "              size_per_head=attention_head_size,\n",
        "              attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
        "              initializer_range=initializer_range,\n",
        "              do_return_2d_tensor=True,\n",
        "              batch_size=batch_size,\n",
        "              from_seq_length=seq_length,\n",
        "              to_seq_length=seq_length)\n",
        "          attention_heads.append(attention_head)\n",
        "\n",
        "        attention_output = None\n",
        "        if len(attention_heads) == 1:\n",
        "          attention_output = attention_heads[0]\n",
        "        else:\n",
        "          # In the case where we have other sequences, we just concatenate\n",
        "          # them to the self-attention head before the projection.\n",
        "          attention_output = tf.concat(attention_heads, axis=-1)\n",
        "\n",
        "        # Run a linear projection of `hidden_size` then add a residual\n",
        "        # with `layer_input`.\n",
        "        with tf.variable_scope(\"output\"):\n",
        "          attention_output = tf.layers.dense(\n",
        "              attention_output,\n",
        "              hidden_size,\n",
        "              kernel_initializer=create_initializer(initializer_range))\n",
        "          attention_output = dropout(attention_output, hidden_dropout_prob)\n",
        "          attention_output = layer_norm(attention_output + layer_input)\n",
        "\n",
        "      # The activation is only applied to the \"intermediate\" hidden layer.\n",
        "      with tf.variable_scope(\"intermediate\"):\n",
        "        intermediate_output = tf.layers.dense(\n",
        "            attention_output,\n",
        "            intermediate_size,\n",
        "            activation=intermediate_act_fn,\n",
        "            kernel_initializer=create_initializer(initializer_range))\n",
        "\n",
        "      # Down-project back to `hidden_size` then add the residual.\n",
        "      with tf.variable_scope(\"output\"):\n",
        "        layer_output = tf.layers.dense(\n",
        "            intermediate_output,\n",
        "            hidden_size,\n",
        "            kernel_initializer=create_initializer(initializer_range))\n",
        "        layer_output = dropout(layer_output, hidden_dropout_prob)\n",
        "        layer_output = layer_norm(layer_output + attention_output)\n",
        "        prev_output = layer_output\n",
        "        all_layer_outputs.append(layer_output)\n",
        "\n",
        "  if do_return_all_layers:\n",
        "    final_outputs = []\n",
        "    for layer_output in all_layer_outputs:\n",
        "      final_output = reshape_from_matrix(layer_output, input_shape)\n",
        "      final_outputs.append(final_output)\n",
        "    return final_outputs\n",
        "  else:\n",
        "    final_output = reshape_from_matrix(prev_output, input_shape)\n",
        "    return final_output\n",
        "\n",
        "\n",
        "def get_shape_list(tensor, expected_rank=None, name=None):\n",
        "  \"\"\"Returns a list of the shape of tensor, preferring static dimensions.\n",
        "\n",
        "  Args:\n",
        "    tensor: A tf.Tensor object to find the shape of.\n",
        "    expected_rank: (optional) int. The expected rank of `tensor`. If this is\n",
        "      specified and the `tensor` has a different rank, and exception will be\n",
        "      thrown.\n",
        "    name: Optional name of the tensor for the error message.\n",
        "\n",
        "  Returns:\n",
        "    A list of dimensions of the shape of tensor. All static dimensions will\n",
        "    be returned as python integers, and dynamic dimensions will be returned\n",
        "    as tf.Tensor scalars.\n",
        "  \"\"\"\n",
        "  if name is None:\n",
        "    name = tensor.name\n",
        "\n",
        "  if expected_rank is not None:\n",
        "    assert_rank(tensor, expected_rank, name)\n",
        "\n",
        "  shape = tensor.shape.as_list()\n",
        "\n",
        "  non_static_indexes = []\n",
        "  for (index, dim) in enumerate(shape):\n",
        "    if dim is None:\n",
        "      non_static_indexes.append(index)\n",
        "\n",
        "  if not non_static_indexes:\n",
        "    return shape\n",
        "\n",
        "  dyn_shape = tf.shape(tensor)\n",
        "  for index in non_static_indexes:\n",
        "    shape[index] = dyn_shape[index]\n",
        "  return shape\n",
        "\n",
        "\n",
        "def reshape_to_matrix(input_tensor):\n",
        "  \"\"\"Reshapes a >= rank 2 tensor to a rank 2 tensor (i.e., a matrix).\"\"\"\n",
        "  ndims = input_tensor.shape.ndims\n",
        "  if ndims < 2:\n",
        "    raise ValueError(\"Input tensor must have at least rank 2. Shape = %s\" %\n",
        "                     (input_tensor.shape))\n",
        "  if ndims == 2:\n",
        "    return input_tensor\n",
        "\n",
        "  width = input_tensor.shape[-1]\n",
        "  output_tensor = tf.reshape(input_tensor, [-1, width])\n",
        "  return output_tensor\n",
        "\n",
        "\n",
        "def reshape_from_matrix(output_tensor, orig_shape_list):\n",
        "  \"\"\"Reshapes a rank 2 tensor back to its original rank >= 2 tensor.\"\"\"\n",
        "  if len(orig_shape_list) == 2:\n",
        "    return output_tensor\n",
        "\n",
        "  output_shape = get_shape_list(output_tensor)\n",
        "\n",
        "  orig_dims = orig_shape_list[0:-1]\n",
        "  width = output_shape[-1]\n",
        "\n",
        "  return tf.reshape(output_tensor, orig_dims + [width])\n",
        "\n",
        "\n",
        "def assert_rank(tensor, expected_rank, name=None):\n",
        "  \"\"\"Raises an exception if the tensor rank is not of the expected rank.\n",
        "\n",
        "  Args:\n",
        "    tensor: A tf.Tensor to check the rank of.\n",
        "    expected_rank: Python integer or list of integers, expected rank.\n",
        "    name: Optional name of the tensor for the error message.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: If the expected shape doesn't match the actual shape.\n",
        "  \"\"\"\n",
        "  if name is None:\n",
        "    name = tensor.name\n",
        "\n",
        "  expected_rank_dict = {}\n",
        "  if isinstance(expected_rank, six.integer_types):\n",
        "    expected_rank_dict[expected_rank] = True\n",
        "  else:\n",
        "    for x in expected_rank:\n",
        "      expected_rank_dict[x] = True\n",
        "\n",
        "  actual_rank = tensor.shape.ndims\n",
        "  if actual_rank not in expected_rank_dict:\n",
        "    scope_name = tf.get_variable_scope().name\n",
        "    raise ValueError(\n",
        "        \"For the tensor `%s` in scope `%s`, the actual rank \"\n",
        "        \"`%d` (shape = %s) is not equal to the expected rank `%s`\" %\n",
        "        (name, scope_name, actual_rank, str(tensor.shape), str(expected_rank)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RKNQ8muOrzw",
        "colab_type": "text"
      },
      "source": [
        "# run_squad.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iyk1bl9TNtI",
        "colab_type": "text"
      },
      "source": [
        "## flags"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji-Dhq5pA2We",
        "colab_type": "text"
      },
      "source": [
        "### flag utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRhGfiO6euwb",
        "colab_type": "code",
        "outputId": "22062d5c-31d3-406d-cd3f-5e277191b04d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "flags = tf.flags\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "def del_all_flags(FLAGS):\n",
        "    flags_dict = FLAGS\n",
        "    keys_list = [keys for keys in flags_dict]\n",
        "    for keys in keys_list:\n",
        "      FLAGS.__delattr__(keys)\n",
        "def printFlags():\n",
        "  flags_dict = FLAGS._flags()\n",
        "  keys_list = [keys for keys in flags_dict]\n",
        "  for keys in keys_list:\n",
        "    FLAGS[keys]\n",
        "def changeFlag(flags, k, t, v, d):\n",
        "  FLAGS = flags.FLAGS\n",
        "  flags_dict = FLAGS._flags()\n",
        "  keys_list = [keys for keys in flags_dict]\n",
        "  if k in keys_list:\n",
        "    FLAGS.__delattr__(k)\n",
        "  if t == \"str\":\n",
        "    flags.DEFINE_string(k, v, d)\n",
        "  elif t == \"int\":\n",
        "    flags.DEFINE_integer(k, v, d)\n",
        "  elif t == \"bool\":\n",
        "    flags.DEFINE_bool(k, v, d)\n",
        "  elif t == \"float\":\n",
        "    flags.DEFINE_float(k, v, d)\n",
        "for key, value in tf.flags.FLAGS.__flags.items():\n",
        "  print(str(key) + \":\" + str(value))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logtostderr:<absl.flags._flag.BooleanFlag object at 0x7fc06b915eb8>\n",
            "alsologtostderr:<absl.flags._flag.BooleanFlag object at 0x7fc06b91f358>\n",
            "log_dir:<absl.flags._flag.Flag object at 0x7fc06b91f400>\n",
            "v:<absl.logging._VerbosityFlag object at 0x7fc06b91f470>\n",
            "verbosity:<absl.logging._VerbosityFlag object at 0x7fc06b91f470>\n",
            "stderrthreshold:<absl.logging._StderrthresholdFlag object at 0x7fc06b91f550>\n",
            "showprefixforinfo:<absl.flags._flag.BooleanFlag object at 0x7fc06b91f668>\n",
            "run_with_pdb:<absl.flags._flag.BooleanFlag object at 0x7fc06b915518>\n",
            "pdb_post_mortem:<absl.flags._flag.BooleanFlag object at 0x7fc06b9eee48>\n",
            "run_with_profiling:<absl.flags._flag.BooleanFlag object at 0x7fc06b9eeeb8>\n",
            "profile_file:<absl.flags._flag.Flag object at 0x7fc06b91fa90>\n",
            "use_cprofile_for_profiling:<absl.flags._flag.BooleanFlag object at 0x7fc06b91fac8>\n",
            "only_check_args:<absl.flags._flag.BooleanFlag object at 0x7fc06b91fb38>\n",
            "op_conversion_fallback_to_while_loop:<absl.flags._flag.BooleanFlag object at 0x7fc06a0882e8>\n",
            "test_random_seed:<absl.flags._flag.Flag object at 0x7fc05e7f5f28>\n",
            "test_srcdir:<absl.flags._flag.Flag object at 0x7fc05e7fe0b8>\n",
            "test_tmpdir:<absl.flags._flag.Flag object at 0x7fc05e7fe2b0>\n",
            "test_randomize_ordering_seed:<absl.flags._flag.Flag object at 0x7fc05e808908>\n",
            "xml_output_file:<absl.flags._flag.Flag object at 0x7fc05e808978>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee-jvzugA771",
        "colab_type": "text"
      },
      "source": [
        "### flag definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uXF79okTMO6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "flags = tf.flags\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "## Required parameters\n",
        "flags.DEFINE_string(\n",
        "    \"bert_config_file\", bert_config_file_path,\n",
        "    \"The config json file corresponding to the pre-trained BERT model. \"\n",
        "    \"This specifies the model architecture.\")\n",
        "\n",
        "flags.DEFINE_string(\"vocab_file\", vocab_file_path,\n",
        "                    \"The vocabulary file that the BERT model was trained on.\")\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"output_dir\", output_dir,\n",
        "    \"The output directory where the model checkpoints will be written.\")\n",
        "\n",
        "## Other parameters\n",
        "flags.DEFINE_string(\"train_file\", AAQAD_train_set_path,\n",
        "                    \"SQuAD json for training. E.g., train-v1.1.json\")\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"predict_file\", AAQAD_dev_set_path,\n",
        "    \"SQuAD json for predictions. E.g., dev-v1.1.json or test-v1.1.json\")\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"init_checkpoint\", bert_init_checkpoint_training_path,\n",
        "    \"Initial checkpoint (usually from a pre-trained BERT model).\")\n",
        "\n",
        "flags.DEFINE_bool(\n",
        "    \"do_lower_case\", False,\n",
        "    \"Whether to lower case the input text. Should be True for uncased \"\n",
        "    \"models and False for cased models.\")\n",
        "\n",
        "flags.DEFINE_integer(\n",
        "    \"max_seq_length\", 384,\n",
        "    \"The maximum total input sequence length after WordPiece tokenization. \"\n",
        "    \"Sequences longer than this will be truncated, and sequences shorter \"\n",
        "    \"than this will be padded.\")\n",
        "\n",
        "flags.DEFINE_integer(\n",
        "    \"doc_stride\", 128,\n",
        "    \"When splitting up a long document into chunks, how much stride to \"\n",
        "    \"take between chunks.\")\n",
        "\n",
        "flags.DEFINE_integer(\n",
        "    \"max_query_length\", 64,\n",
        "    \"The maximum number of tokens for the question. Questions longer than \"\n",
        "    \"this will be truncated to this length.\")\n",
        "\n",
        "flags.DEFINE_bool(\"do_train\", True, \"Whether to run training.\")\n",
        "\n",
        "flags.DEFINE_bool(\"do_predict\", True, \"Whether to run eval on the dev set.\")\n",
        "\n",
        "flags.DEFINE_integer(\"train_batch_size\", 8, \"Total batch size for training.\")\n",
        "\n",
        "flags.DEFINE_integer(\"predict_batch_size\", 8,\n",
        "                     \"Total batch size for predictions.\")\n",
        "\n",
        "flags.DEFINE_float(\"learning_rate\", 5e-5, \"The initial learning rate for Adam.\")\n",
        "\n",
        "flags.DEFINE_float(\"num_train_epochs\", 3,\n",
        "                   \"Total number of training epochs to perform.\")\n",
        "\n",
        "flags.DEFINE_float(\n",
        "    \"warmup_proportion\", 0.1,\n",
        "    \"Proportion of training to perform linear learning rate warmup for. \"\n",
        "    \"E.g., 0.1 = 10% of training.\")\n",
        "\n",
        "flags.DEFINE_integer(\"save_checkpoints_steps\", 1000,\n",
        "                     \"How often to save the model checkpoint.\")\n",
        "\n",
        "flags.DEFINE_integer(\"iterations_per_loop\", 1000,\n",
        "                     \"How many steps to make in each estimator call.\")\n",
        "\n",
        "flags.DEFINE_integer(\n",
        "    \"n_best_size\", 20,\n",
        "    \"The total number of n-best predictions to generate in the \"\n",
        "    \"nbest_predictions.json output file.\")\n",
        "\n",
        "flags.DEFINE_integer(\n",
        "    \"max_answer_length\", 30,\n",
        "    \"The maximum length of an answer that can be generated. This is needed \"\n",
        "    \"because the start and end predictions are not conditioned on one another.\")\n",
        "\n",
        "flags.DEFINE_bool(\"use_tpu\", False, \"Whether to use TPU or GPU/CPU.\")\n",
        "\n",
        "tf.flags.DEFINE_string(\n",
        "    \"tpu_name\", None,\n",
        "    \"The Cloud TPU to use for training. This should be either the name \"\n",
        "    \"used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 \"\n",
        "    \"url.\")\n",
        "\n",
        "tf.flags.DEFINE_string(\n",
        "    \"tpu_zone\", None,\n",
        "    \"[Optional] GCE zone where the Cloud TPU is located in. If not \"\n",
        "    \"specified, we will attempt to automatically detect the GCE project from \"\n",
        "    \"metadata.\")\n",
        "\n",
        "tf.flags.DEFINE_string(\n",
        "    \"gcp_project\", None,\n",
        "    \"[Optional] Project name for the Cloud TPU-enabled project. If not \"\n",
        "    \"specified, we will attempt to automatically detect the GCE project from \"\n",
        "    \"metadata.\")\n",
        "tf.flags.DEFINE_string(\n",
        "    \"f\", None,\n",
        "    \"dummy parameter for sending file in colab notebook\")\n",
        "\n",
        "tf.flags.DEFINE_string(\"master\", None, \"[Optional] TensorFlow master URL.\")\n",
        "\n",
        "flags.DEFINE_integer(\n",
        "    \"num_tpu_cores\", 8,\n",
        "    \"Only used if `use_tpu` is True. Total number of TPU cores to use.\")\n",
        "\n",
        "flags.DEFINE_bool(\n",
        "    \"verbose_logging\", False,\n",
        "    \"If true, all of the warnings related to data processing will be printed. \"\n",
        "    \"A number of warnings are expected for a normal SQuAD evaluation.\")\n",
        "\n",
        "flags.DEFINE_bool(\n",
        "    \"version_2_with_negative\", True,\n",
        "    \"If true, the SQuAD examples contain some that do not have an answer.\")\n",
        "\n",
        "flags.DEFINE_float(\n",
        "    \"null_score_diff_threshold\", 0.0,\n",
        "    \"If null_score - best_non_null is greater than the threshold predict null.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dqd68RQubJ76",
        "colab_type": "text"
      },
      "source": [
        "# change parameters for"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgf0EDTIzU02",
        "colab_type": "text"
      },
      "source": [
        "## predictions only"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkwqNHaSzgwq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "changeFlag(tf.flags, \"predict_file\", \"str\", AAQAD_test_set_path, \"SQuAD json for predictions. E.g., dev-v1.1.json or test-v1.1.json.\")\n",
        "changeFlag(tf.flags, \"do_train\", \"bool\", False, \"Whether to run training.\")\n",
        "#Note: you can change any flags here to suit your purpos see flag definitions above that you can change"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxlM36JP3gv_",
        "colab_type": "text"
      },
      "source": [
        "## in case of OOM error lower batch size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcSDmJLl3VrA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "changeFlag(tf.flags, \"train_batch_size\", \"int\", 8, \"Total batch size for training.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Z1NPE0PTS1M",
        "colab_type": "text"
      },
      "source": [
        "# run squad implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfbXRQMEOhgx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"Run BERT on SQuAD 1.1 and SQuAD 2.0.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import six\n",
        "\n",
        "\n",
        "class SquadExample(object):\n",
        "  \"\"\"A single training/test example for simple sequence classification.\n",
        "\n",
        "     For examples without an answer, the start and end position are -1.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               qas_id,\n",
        "               question_text,\n",
        "               doc_tokens,\n",
        "               orig_answer_text=None,\n",
        "               start_position=None,\n",
        "               end_position=None,\n",
        "               is_impossible=False):\n",
        "    self.qas_id = qas_id\n",
        "    self.question_text = question_text\n",
        "    self.doc_tokens = doc_tokens\n",
        "    self.orig_answer_text = orig_answer_text\n",
        "    self.start_position = start_position\n",
        "    self.end_position = end_position\n",
        "    self.is_impossible = is_impossible\n",
        "\n",
        "  def __str__(self):\n",
        "    return self.__repr__()\n",
        "\n",
        "  def __repr__(self):\n",
        "    s = \"\"\n",
        "    s += \"qas_id: %s\" % (printable_text(self.qas_id))\n",
        "    s += \", question_text: %s\" % (\n",
        "        printable_text(self.question_text))\n",
        "    s += \", doc_tokens: [%s]\" % (\" \".join(self.doc_tokens))\n",
        "    if self.start_position:\n",
        "      s += \", start_position: %d\" % (self.start_position)\n",
        "    if self.start_position:\n",
        "      s += \", end_position: %d\" % (self.end_position)\n",
        "    if self.start_position:\n",
        "      s += \", is_impossible: %r\" % (self.is_impossible)\n",
        "    return s\n",
        "\n",
        "\n",
        "class InputFeatures(object):\n",
        "  \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               unique_id,\n",
        "               example_index,\n",
        "               doc_span_index,\n",
        "               tokens,\n",
        "               token_to_orig_map,\n",
        "               token_is_max_context,\n",
        "               input_ids,\n",
        "               input_mask,\n",
        "               segment_ids,\n",
        "               start_position=None,\n",
        "               end_position=None,\n",
        "               is_impossible=None):\n",
        "    self.unique_id = unique_id\n",
        "    self.example_index = example_index\n",
        "    self.doc_span_index = doc_span_index\n",
        "    self.tokens = tokens\n",
        "    self.token_to_orig_map = token_to_orig_map\n",
        "    self.token_is_max_context = token_is_max_context\n",
        "    self.input_ids = input_ids\n",
        "    self.input_mask = input_mask\n",
        "    self.segment_ids = segment_ids\n",
        "    self.start_position = start_position\n",
        "    self.end_position = end_position\n",
        "    self.is_impossible = is_impossible\n",
        "\n",
        "\n",
        "def read_squad_examples(input_file, is_training):\n",
        "  \"\"\"Read a SQuAD json file into a list of SquadExample.\"\"\"\n",
        "  with tf.gfile.Open(input_file, \"r\") as reader:\n",
        "    input_data = json.load(reader)[\"data\"]\n",
        "\n",
        "  def is_whitespace(c):\n",
        "    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
        "      return True\n",
        "    return False\n",
        "\n",
        "  examples = []\n",
        "  for entry in input_data:\n",
        "    for paragraph in entry[\"paragraphs\"]:\n",
        "      paragraph_text = paragraph[\"context\"]\n",
        "      doc_tokens = []\n",
        "      char_to_word_offset = []\n",
        "      prev_is_whitespace = True\n",
        "      for c in paragraph_text:\n",
        "        if is_whitespace(c):\n",
        "          prev_is_whitespace = True\n",
        "        else:\n",
        "          if prev_is_whitespace:\n",
        "            doc_tokens.append(c)\n",
        "          else:\n",
        "            doc_tokens[-1] += c\n",
        "          prev_is_whitespace = False\n",
        "        char_to_word_offset.append(len(doc_tokens) - 1)\n",
        "\n",
        "      for qa in paragraph[\"qas\"]:\n",
        "        qas_id = qa[\"id\"]\n",
        "        question_text = qa[\"question\"]\n",
        "        start_position = None\n",
        "        end_position = None\n",
        "        orig_answer_text = None\n",
        "        is_impossible = False\n",
        "        if is_training:     \n",
        "          if FLAGS.version_2_with_negative:\n",
        "            is_impossible = qa[\"is_impossible\"]\n",
        "          #if not FLAGS.version_2_with_negative and qa[\"is_impossible\"]:#added by AQAAD\n",
        "          #  continue\n",
        "          if (len(qa[\"answers\"]) != 1) and (not is_impossible):\n",
        "            raise ValueError(\n",
        "                \"For training, each question should have exactly 1 answer.\")\n",
        "          if not is_impossible:\n",
        "            answer = qa[\"answers\"][0]\n",
        "            orig_answer_text = answer[\"text\"]\n",
        "            answer_offset = answer[\"answer_start\"]\n",
        "            answer_length = len(orig_answer_text)\n",
        "            start_position = char_to_word_offset[answer_offset]\n",
        "            end_position = char_to_word_offset[answer_offset + answer_length -\n",
        "                                               1]\n",
        "            # Only add answers where the text can be exactly recovered from the\n",
        "            # document. If this CAN'T happen it's likely due to weird Unicode\n",
        "            # stuff so we will just skip the example.\n",
        "            #\n",
        "            # Note that this means for training mode, every example is NOT\n",
        "            # guaranteed to be preserved.\n",
        "            actual_text = \" \".join(\n",
        "                doc_tokens[start_position:(end_position + 1)])\n",
        "            cleaned_answer_text = \" \".join(\n",
        "                whitespace_tokenize(orig_answer_text))\n",
        "            if actual_text.find(cleaned_answer_text) == -1:\n",
        "              tf.logging.warning(\"Could not find answer: '%s' vs. '%s'\",\n",
        "                                 actual_text, cleaned_answer_text)\n",
        "              continue\n",
        "          else:\n",
        "            start_position = -1\n",
        "            end_position = -1\n",
        "            orig_answer_text = \"\"\n",
        "\n",
        "        example = SquadExample(\n",
        "            qas_id=qas_id,\n",
        "            question_text=question_text,\n",
        "            doc_tokens=doc_tokens,\n",
        "            orig_answer_text=orig_answer_text,\n",
        "            start_position=start_position,\n",
        "            end_position=end_position,\n",
        "            is_impossible=is_impossible)\n",
        "        examples.append(example)\n",
        "\n",
        "  return examples\n",
        "\n",
        "\n",
        "def convert_examples_to_features(examples, tokenizer, max_seq_length,\n",
        "                                 doc_stride, max_query_length, is_training,\n",
        "                                 output_fn):\n",
        "  \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "\n",
        "  unique_id = 1000000000\n",
        "\n",
        "  for (example_index, example) in enumerate(examples):\n",
        "    query_tokens = tokenizer.tokenize(example.question_text)\n",
        "\n",
        "    if len(query_tokens) > max_query_length:\n",
        "      query_tokens = query_tokens[0:max_query_length]\n",
        "\n",
        "    tok_to_orig_index = []\n",
        "    orig_to_tok_index = []\n",
        "    all_doc_tokens = []\n",
        "    for (i, token) in enumerate(example.doc_tokens):\n",
        "      orig_to_tok_index.append(len(all_doc_tokens))\n",
        "      sub_tokens = tokenizer.tokenize(token)\n",
        "      for sub_token in sub_tokens:\n",
        "        tok_to_orig_index.append(i)\n",
        "        all_doc_tokens.append(sub_token)\n",
        "\n",
        "    tok_start_position = None\n",
        "    tok_end_position = None\n",
        "    if is_training and example.is_impossible:\n",
        "      tok_start_position = -1\n",
        "      tok_end_position = -1\n",
        "    if is_training and not example.is_impossible:\n",
        "      tok_start_position = orig_to_tok_index[example.start_position]\n",
        "      if example.end_position < len(example.doc_tokens) - 1:\n",
        "        tok_end_position = orig_to_tok_index[example.end_position + 1] - 1\n",
        "      else:\n",
        "        tok_end_position = len(all_doc_tokens) - 1\n",
        "      (tok_start_position, tok_end_position) = _improve_answer_span(\n",
        "          all_doc_tokens, tok_start_position, tok_end_position, tokenizer,\n",
        "          example.orig_answer_text)\n",
        "\n",
        "    # The -3 accounts for [CLS], [SEP] and [SEP]\n",
        "    max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n",
        "\n",
        "    # We can have documents that are longer than the maximum sequence length.\n",
        "    # To deal with this we do a sliding window approach, where we take chunks\n",
        "    # of the up to our max length with a stride of `doc_stride`.\n",
        "    _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n",
        "        \"DocSpan\", [\"start\", \"length\"])\n",
        "    doc_spans = []\n",
        "    start_offset = 0\n",
        "    while start_offset < len(all_doc_tokens):\n",
        "      length = len(all_doc_tokens) - start_offset\n",
        "      if length > max_tokens_for_doc:\n",
        "        length = max_tokens_for_doc\n",
        "      doc_spans.append(_DocSpan(start=start_offset, length=length))\n",
        "      if start_offset + length == len(all_doc_tokens):\n",
        "        break\n",
        "      start_offset += min(length, doc_stride)\n",
        "\n",
        "    for (doc_span_index, doc_span) in enumerate(doc_spans):\n",
        "      tokens = []\n",
        "      token_to_orig_map = {}\n",
        "      token_is_max_context = {}\n",
        "      segment_ids = []\n",
        "      tokens.append(\"[CLS]\")\n",
        "      segment_ids.append(0)\n",
        "      for token in query_tokens:\n",
        "        tokens.append(token)\n",
        "        segment_ids.append(0)\n",
        "      tokens.append(\"[SEP]\")\n",
        "      segment_ids.append(0)\n",
        "\n",
        "      for i in range(doc_span.length):\n",
        "        split_token_index = doc_span.start + i\n",
        "        token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n",
        "\n",
        "        is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n",
        "                                               split_token_index)\n",
        "        token_is_max_context[len(tokens)] = is_max_context\n",
        "        tokens.append(all_doc_tokens[split_token_index])\n",
        "        segment_ids.append(1)\n",
        "      tokens.append(\"[SEP]\")\n",
        "      segment_ids.append(1)\n",
        "\n",
        "      input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "      # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "      # tokens are attended to.\n",
        "      input_mask = [1] * len(input_ids)\n",
        "\n",
        "      # Zero-pad up to the sequence length.\n",
        "      while len(input_ids) < max_seq_length:\n",
        "        input_ids.append(0)\n",
        "        input_mask.append(0)\n",
        "        segment_ids.append(0)\n",
        "\n",
        "      assert len(input_ids) == max_seq_length\n",
        "      assert len(input_mask) == max_seq_length\n",
        "      assert len(segment_ids) == max_seq_length\n",
        "\n",
        "      start_position = None\n",
        "      end_position = None\n",
        "      if is_training and not example.is_impossible:\n",
        "        # For training, if our document chunk does not contain an annotation\n",
        "        # we throw it out, since there is nothing to predict.\n",
        "        doc_start = doc_span.start\n",
        "        doc_end = doc_span.start + doc_span.length - 1\n",
        "        out_of_span = False\n",
        "        if not (tok_start_position >= doc_start and\n",
        "                tok_end_position <= doc_end):\n",
        "          out_of_span = True\n",
        "        if out_of_span:\n",
        "          start_position = 0\n",
        "          end_position = 0\n",
        "        else:\n",
        "          doc_offset = len(query_tokens) + 2\n",
        "          start_position = tok_start_position - doc_start + doc_offset\n",
        "          end_position = tok_end_position - doc_start + doc_offset\n",
        "\n",
        "      if is_training and example.is_impossible:\n",
        "        start_position = 0\n",
        "        end_position = 0\n",
        "\n",
        "      if example_index < 20:\n",
        "        tf.logging.info(\"*** Example ***\")\n",
        "        tf.logging.info(\"unique_id: %s\" % (unique_id))\n",
        "        tf.logging.info(\"example_index: %s\" % (example_index))\n",
        "        tf.logging.info(\"doc_span_index: %s\" % (doc_span_index))\n",
        "        tf.logging.info(\"tokens: %s\" % \" \".join(\n",
        "            [printable_text(x) for x in tokens]))\n",
        "        tf.logging.info(\"token_to_orig_map: %s\" % \" \".join(\n",
        "            [\"%d:%d\" % (x, y) for (x, y) in six.iteritems(token_to_orig_map)]))\n",
        "        tf.logging.info(\"token_is_max_context: %s\" % \" \".join([\n",
        "            \"%d:%s\" % (x, y) for (x, y) in six.iteritems(token_is_max_context)\n",
        "        ]))\n",
        "        tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "        tf.logging.info(\n",
        "            \"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "        tf.logging.info(\n",
        "            \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
        "        if is_training and example.is_impossible:\n",
        "          tf.logging.info(\"impossible example\")\n",
        "        if is_training and not example.is_impossible:\n",
        "          answer_text = \" \".join(tokens[start_position:(end_position + 1)])\n",
        "          tf.logging.info(\"start_position: %d\" % (start_position))\n",
        "          tf.logging.info(\"end_position: %d\" % (end_position))\n",
        "          tf.logging.info(\n",
        "              \"answer: %s\" % (printable_text(answer_text)))\n",
        "\n",
        "      feature = InputFeatures(\n",
        "          unique_id=unique_id,\n",
        "          example_index=example_index,\n",
        "          doc_span_index=doc_span_index,\n",
        "          tokens=tokens,\n",
        "          token_to_orig_map=token_to_orig_map,\n",
        "          token_is_max_context=token_is_max_context,\n",
        "          input_ids=input_ids,\n",
        "          input_mask=input_mask,\n",
        "          segment_ids=segment_ids,\n",
        "          start_position=start_position,\n",
        "          end_position=end_position,\n",
        "          is_impossible=example.is_impossible)\n",
        "\n",
        "      # Run callback\n",
        "      output_fn(feature)\n",
        "\n",
        "      unique_id += 1\n",
        "\n",
        "\n",
        "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n",
        "                         orig_answer_text):\n",
        "  \"\"\"Returns tokenized answer spans that better match the annotated answer.\"\"\"\n",
        "\n",
        "  # The SQuAD annotations are character based. We first project them to\n",
        "  # whitespace-tokenized words. But then after WordPiece tokenization, we can\n",
        "  # often find a \"better match\". For example:\n",
        "  #\n",
        "  #   Question: What year was John Smith born?\n",
        "  #   Context: The leader was John Smith (1895-1943).\n",
        "  #   Answer: 1895\n",
        "  #\n",
        "  # The original whitespace-tokenized answer will be \"(1895-1943).\". However\n",
        "  # after tokenization, our tokens will be \"( 1895 - 1943 ) .\". So we can match\n",
        "  # the exact answer, 1895.\n",
        "  #\n",
        "  # However, this is not always possible. Consider the following:\n",
        "  #\n",
        "  #   Question: What country is the top exporter of electornics?\n",
        "  #   Context: The Japanese electronics industry is the lagest in the world.\n",
        "  #   Answer: Japan\n",
        "  #\n",
        "  # In this case, the annotator chose \"Japan\" as a character sub-span of\n",
        "  # the word \"Japanese\". Since our WordPiece tokenizer does not split\n",
        "  # \"Japanese\", we just use \"Japanese\" as the annotation. This is fairly rare\n",
        "  # in SQuAD, but does happen.\n",
        "  tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n",
        "\n",
        "  for new_start in range(input_start, input_end + 1):\n",
        "    for new_end in range(input_end, new_start - 1, -1):\n",
        "      text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n",
        "      if text_span == tok_answer_text:\n",
        "        return (new_start, new_end)\n",
        "\n",
        "  return (input_start, input_end)\n",
        "\n",
        "\n",
        "def _check_is_max_context(doc_spans, cur_span_index, position):\n",
        "  \"\"\"Check if this is the 'max context' doc span for the token.\"\"\"\n",
        "\n",
        "  # Because of the sliding window approach taken to scoring documents, a single\n",
        "  # token can appear in multiple documents. E.g.\n",
        "  #  Doc: the man went to the store and bought a gallon of milk\n",
        "  #  Span A: the man went to the\n",
        "  #  Span B: to the store and bought\n",
        "  #  Span C: and bought a gallon of\n",
        "  #  ...\n",
        "  #\n",
        "  # Now the word 'bought' will have two scores from spans B and C. We only\n",
        "  # want to consider the score with \"maximum context\", which we define as\n",
        "  # the *minimum* of its left and right context (the *sum* of left and\n",
        "  # right context will always be the same, of course).\n",
        "  #\n",
        "  # In the example the maximum context for 'bought' would be span C since\n",
        "  # it has 1 left context and 3 right context, while span B has 4 left context\n",
        "  # and 0 right context.\n",
        "  best_score = None\n",
        "  best_span_index = None\n",
        "  for (span_index, doc_span) in enumerate(doc_spans):\n",
        "    end = doc_span.start + doc_span.length - 1\n",
        "    if position < doc_span.start:\n",
        "      continue\n",
        "    if position > end:\n",
        "      continue\n",
        "    num_left_context = position - doc_span.start\n",
        "    num_right_context = end - position\n",
        "    score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n",
        "    if best_score is None or score > best_score:\n",
        "      best_score = score\n",
        "      best_span_index = span_index\n",
        "\n",
        "  return cur_span_index == best_span_index\n",
        "\n",
        "\n",
        "def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,\n",
        "                 use_one_hot_embeddings):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "  model = BertModel(\n",
        "      config=bert_config,\n",
        "      is_training=is_training,\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      token_type_ids=segment_ids,\n",
        "      use_one_hot_embeddings=use_one_hot_embeddings)\n",
        "\n",
        "  final_hidden = model.get_sequence_output()\n",
        "\n",
        "  final_hidden_shape = get_shape_list(final_hidden, expected_rank=3)\n",
        "  batch_size = final_hidden_shape[0]\n",
        "  seq_length = final_hidden_shape[1]\n",
        "  hidden_size = final_hidden_shape[2]\n",
        "\n",
        "  output_weights = tf.get_variable(\n",
        "      \"cls/squad/output_weights\", [2, hidden_size],\n",
        "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "  output_bias = tf.get_variable(\n",
        "      \"cls/squad/output_bias\", [2], initializer=tf.zeros_initializer())\n",
        "\n",
        "  final_hidden_matrix = tf.reshape(final_hidden,\n",
        "                                   [batch_size * seq_length, hidden_size])\n",
        "  logits = tf.matmul(final_hidden_matrix, output_weights, transpose_b=True)\n",
        "  logits = tf.nn.bias_add(logits, output_bias)\n",
        "\n",
        "  logits = tf.reshape(logits, [batch_size, seq_length, 2])\n",
        "  logits = tf.transpose(logits, [2, 0, 1])\n",
        "\n",
        "  unstacked_logits = tf.unstack(logits, axis=0)\n",
        "\n",
        "  (start_logits, end_logits) = (unstacked_logits[0], unstacked_logits[1])\n",
        "\n",
        "  return (start_logits, end_logits)\n",
        "\n",
        "\n",
        "def model_fn_builder(bert_config, init_checkpoint, learning_rate,\n",
        "                     num_train_steps, num_warmup_steps, use_tpu,\n",
        "                     use_one_hot_embeddings):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "\n",
        "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    tf.logging.info(\"*** Features ***\")\n",
        "    for name in sorted(features.keys()):\n",
        "      tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
        "\n",
        "    unique_ids = features[\"unique_ids\"]\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "\n",
        "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "\n",
        "    (start_logits, end_logits) = create_model(\n",
        "        bert_config=bert_config,\n",
        "        is_training=is_training,\n",
        "        input_ids=input_ids,\n",
        "        input_mask=input_mask,\n",
        "        segment_ids=segment_ids,\n",
        "        use_one_hot_embeddings=use_one_hot_embeddings)\n",
        "\n",
        "    tvars = tf.trainable_variables()\n",
        "\n",
        "    initialized_variable_names = {}\n",
        "    scaffold_fn = None\n",
        "    if init_checkpoint:\n",
        "      (assignment_map, initialized_variable_names\n",
        "      ) = get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
        "      if use_tpu:\n",
        "\n",
        "        def tpu_scaffold():\n",
        "          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "          return tf.train.Scaffold()\n",
        "\n",
        "        scaffold_fn = tpu_scaffold\n",
        "      else:\n",
        "        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "\n",
        "    tf.logging.info(\"**** Trainable Variables ****\")\n",
        "    for var in tvars:\n",
        "      init_string = \"\"\n",
        "      if var.name in initialized_variable_names:\n",
        "        init_string = \", *INIT_FROM_CKPT*\"\n",
        "      tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
        "                      init_string)\n",
        "\n",
        "    output_spec = None\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "      seq_length = get_shape_list(input_ids)[1]\n",
        "\n",
        "      def compute_loss(logits, positions):\n",
        "        one_hot_positions = tf.one_hot(\n",
        "            positions, depth=seq_length, dtype=tf.float32)\n",
        "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "        loss = -tf.reduce_mean(\n",
        "            tf.reduce_sum(one_hot_positions * log_probs, axis=-1))\n",
        "        return loss\n",
        "\n",
        "      start_positions = features[\"start_positions\"]\n",
        "      end_positions = features[\"end_positions\"]\n",
        "\n",
        "      start_loss = compute_loss(start_logits, start_positions)\n",
        "      end_loss = compute_loss(end_logits, end_positions)\n",
        "\n",
        "      total_loss = (start_loss + end_loss) / 2.0\n",
        "\n",
        "      train_op = create_optimizer(\n",
        "          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n",
        "\n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode,\n",
        "          loss=total_loss,\n",
        "          train_op=train_op,\n",
        "          scaffold_fn=scaffold_fn)\n",
        "    elif mode == tf.estimator.ModeKeys.PREDICT:\n",
        "      predictions = {\n",
        "          \"unique_ids\": unique_ids,\n",
        "          \"start_logits\": start_logits,\n",
        "          \"end_logits\": end_logits,\n",
        "      }\n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode, predictions=predictions, scaffold_fn=scaffold_fn)\n",
        "    else:\n",
        "      raise ValueError(\n",
        "          \"Only TRAIN and PREDICT modes are supported: %s\" % (mode))\n",
        "output_spec\n",
        "    return output_spec\n",
        "\n",
        "  return model_fn\n",
        "\n",
        "\n",
        "def input_fn_builder(input_file, seq_length, is_training, drop_remainder):\n",
        "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
        "\n",
        "  name_to_features = {\n",
        "      \"unique_ids\": tf.FixedLenFeature([], tf.int64),\n",
        "      \"input_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
        "      \"input_mask\": tf.FixedLenFeature([seq_length], tf.int64),\n",
        "      \"segment_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
        "  }\n",
        "\n",
        "  if is_training:\n",
        "    name_to_features[\"start_positions\"] = tf.FixedLenFeature([], tf.int64)\n",
        "    name_to_features[\"end_positions\"] = tf.FixedLenFeature([], tf.int64)\n",
        "\n",
        "  def _decode_record(record, name_to_features):\n",
        "    \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
        "    example = tf.parse_single_example(record, name_to_features)\n",
        "\n",
        "    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
        "    # So cast all int64 to int32.\n",
        "    for name in list(example.keys()):\n",
        "      t = example[name]\n",
        "      if t.dtype == tf.int64:\n",
        "        t = tf.to_int32(t)\n",
        "      example[name] = t\n",
        "\n",
        "    return example\n",
        "\n",
        "  def input_fn(params):\n",
        "    \"\"\"The actual input function.\"\"\"\n",
        "    batch_size = params[\"batch_size\"]\n",
        "\n",
        "    # For training, we want a lot of parallel reading and shuffling.\n",
        "    # For eval, we want no shuffling and parallel reading doesn't matter.\n",
        "    d = tf.data.TFRecordDataset(input_file)\n",
        "    if is_training:\n",
        "      d = d.repeat()\n",
        "      d = d.shuffle(buffer_size=100)\n",
        "\n",
        "    d = d.apply(\n",
        "        tf.contrib.data.map_and_batch(\n",
        "            lambda record: _decode_record(record, name_to_features),\n",
        "            batch_size=batch_size,\n",
        "            drop_remainder=drop_remainder))\n",
        "\n",
        "    return d\n",
        "\n",
        "  return input_fn\n",
        "\n",
        "\n",
        "RawResult = collections.namedtuple(\"RawResult\",\n",
        "                                   [\"unique_id\", \"start_logits\", \"end_logits\"])\n",
        "\n",
        "\n",
        "def write_predictions(all_examples, all_features, all_results, n_best_size,\n",
        "                      max_answer_length, do_lower_case, output_prediction_file,\n",
        "                      output_nbest_file, output_null_log_odds_file):\n",
        "  \"\"\"Write final predictions to the json file and log-odds of null if needed.\"\"\"\n",
        "  tf.logging.info(\"Writing predictions to: %s\" % (output_prediction_file))\n",
        "  tf.logging.info(\"Writing nbest to: %s\" % (output_nbest_file))\n",
        "\n",
        "  example_index_to_features = collections.defaultdict(list)\n",
        "  for feature in all_features:\n",
        "    example_index_to_features[feature.example_index].append(feature)\n",
        "\n",
        "  unique_id_to_result = {}\n",
        "  for result in all_results:\n",
        "    unique_id_to_result[result.unique_id] = result\n",
        "\n",
        "  _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
        "      \"PrelimPrediction\",\n",
        "      [\"feature_index\", \"start_index\", \"end_index\", \"start_logit\", \"end_logit\"])\n",
        "\n",
        "  all_predictions = collections.OrderedDict()\n",
        "  all_nbest_json = collections.OrderedDict()\n",
        "  scores_diff_json = collections.OrderedDict()\n",
        "\n",
        "  for (example_index, example) in enumerate(all_examples):\n",
        "    features = example_index_to_features[example_index]\n",
        "\n",
        "    prelim_predictions = []\n",
        "    # keep track of the minimum score of null start+end of position 0\n",
        "    score_null = 1000000  # large and positive\n",
        "    min_null_feature_index = 0  # the paragraph slice with min mull score\n",
        "    null_start_logit = 0  # the start logit at the slice with min null score\n",
        "    null_end_logit = 0  # the end logit at the slice with min null score\n",
        "    for (feature_index, feature) in enumerate(features):\n",
        "      result = unique_id_to_result[feature.unique_id]\n",
        "      start_indexes = _get_best_indexes(result.start_logits, n_best_size)\n",
        "      end_indexes = _get_best_indexes(result.end_logits, n_best_size)\n",
        "      # if we could have irrelevant answers, get the min score of irrelevant\n",
        "      if FLAGS.version_2_with_negative:\n",
        "        feature_null_score = result.start_logits[0] + result.end_logits[0]\n",
        "        if feature_null_score < score_null:\n",
        "          score_null = feature_null_score\n",
        "          min_null_feature_index = feature_index\n",
        "          null_start_logit = result.start_logits[0]\n",
        "          null_end_logit = result.end_logits[0]\n",
        "      for start_index in start_indexes:\n",
        "        for end_index in end_indexes:\n",
        "          # We could hypothetically create invalid predictions, e.g., predict\n",
        "          # that the start of the span is in the question. We throw out all\n",
        "          # invalid predictions.\n",
        "          if start_index >= len(feature.tokens):\n",
        "            continue\n",
        "          if end_index >= len(feature.tokens):\n",
        "            continue\n",
        "          if start_index not in feature.token_to_orig_map:\n",
        "            continue\n",
        "          if end_index not in feature.token_to_orig_map:\n",
        "            continue\n",
        "          if not feature.token_is_max_context.get(start_index, False):\n",
        "            continue\n",
        "          if end_index < start_index:\n",
        "            continue\n",
        "          length = end_index - start_index + 1\n",
        "          if length > max_answer_length:\n",
        "            continue\n",
        "          prelim_predictions.append(\n",
        "              _PrelimPrediction(\n",
        "                  feature_index=feature_index,\n",
        "                  start_index=start_index,\n",
        "                  end_index=end_index,\n",
        "                  start_logit=result.start_logits[start_index],\n",
        "                  end_logit=result.end_logits[end_index]))\n",
        "\n",
        "    if FLAGS.version_2_with_negative:\n",
        "      prelim_predictions.append(\n",
        "          _PrelimPrediction(\n",
        "              feature_index=min_null_feature_index,\n",
        "              start_index=0,\n",
        "              end_index=0,\n",
        "              start_logit=null_start_logit,\n",
        "              end_logit=null_end_logit))\n",
        "    prelim_predictions = sorted(\n",
        "        prelim_predictions,\n",
        "        key=lambda x: (x.start_logit + x.end_logit),\n",
        "        reverse=True)\n",
        "\n",
        "    _NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
        "        \"NbestPrediction\", [\"text\", \"start_logit\", \"end_logit\"])\n",
        "\n",
        "    seen_predictions = {}\n",
        "    nbest = []\n",
        "    for pred in prelim_predictions:\n",
        "      if len(nbest) >= n_best_size:\n",
        "        break\n",
        "      feature = features[pred.feature_index]\n",
        "      if pred.start_index > 0:  # this is a non-null prediction\n",
        "        tok_tokens = feature.tokens[pred.start_index:(pred.end_index + 1)]\n",
        "        orig_doc_start = feature.token_to_orig_map[pred.start_index]\n",
        "        orig_doc_end = feature.token_to_orig_map[pred.end_index]\n",
        "        orig_tokens = example.doc_tokens[orig_doc_start:(orig_doc_end + 1)]\n",
        "        tok_text = \" \".join(tok_tokens)\n",
        "\n",
        "        # De-tokenize WordPieces that have been split off.\n",
        "        tok_text = tok_text.replace(\" ##\", \"\")\n",
        "        tok_text = tok_text.replace(\"##\", \"\")\n",
        "\n",
        "        # Clean whitespace\n",
        "        tok_text = tok_text.strip()\n",
        "        tok_text = \" \".join(tok_text.split())\n",
        "        orig_text = \" \".join(orig_tokens)\n",
        "\n",
        "        final_text = get_final_text(tok_text, orig_text, do_lower_case)\n",
        "        if final_text in seen_predictions:\n",
        "          continue\n",
        "\n",
        "        seen_predictions[final_text] = True\n",
        "      else:\n",
        "        final_text = \"\"\n",
        "        seen_predictions[final_text] = True\n",
        "\n",
        "      nbest.append(\n",
        "          _NbestPrediction(\n",
        "              text=final_text,\n",
        "              start_logit=pred.start_logit,\n",
        "              end_logit=pred.end_logit))\n",
        "\n",
        "    # if we didn't inlude the empty option in the n-best, inlcude it\n",
        "    if FLAGS.version_2_with_negative:\n",
        "      if \"\" not in seen_predictions:\n",
        "        nbest.append(\n",
        "            _NbestPrediction(\n",
        "                text=\"\", start_logit=null_start_logit,\n",
        "                end_logit=null_end_logit))\n",
        "    # In very rare edge cases we could have no valid predictions. So we\n",
        "    # just create a nonce prediction in this case to avoid failure.\n",
        "    if not nbest:\n",
        "      nbest.append(\n",
        "          _NbestPrediction(text=\"empty\", start_logit=0.0, end_logit=0.0))\n",
        "\n",
        "    assert len(nbest) >= 1\n",
        "\n",
        "    total_scores = []\n",
        "    best_non_null_entry = None\n",
        "    for entry in nbest:\n",
        "      total_scores.append(entry.start_logit + entry.end_logit)\n",
        "      if not best_non_null_entry:\n",
        "        if entry.text:\n",
        "          best_non_null_entry = entry\n",
        "\n",
        "    probs = _compute_softmax(total_scores)\n",
        "\n",
        "    nbest_json = []\n",
        "    for (i, entry) in enumerate(nbest):\n",
        "      output = collections.OrderedDict()\n",
        "      output[\"text\"] = entry.text\n",
        "      output[\"probability\"] = probs[i]\n",
        "      output[\"start_logit\"] = entry.start_logit\n",
        "      output[\"end_logit\"] = entry.end_logit\n",
        "      nbest_json.append(output)\n",
        "\n",
        "    assert len(nbest_json) >= 1\n",
        "\n",
        "    if not FLAGS.version_2_with_negative:\n",
        "      all_predictions[example.qas_id] = nbest_json[0][\"text\"]\n",
        "    else:\n",
        "      # predict \"\" iff the null score - the score of best non-null > threshold\n",
        "      score_diff = score_null - best_non_null_entry.start_logit - (\n",
        "          best_non_null_entry.end_logit)\n",
        "      scores_diff_json[example.qas_id] = score_diff\n",
        "      if score_diff > FLAGS.null_score_diff_threshold:\n",
        "        all_predictions[example.qas_id] = \"\"\n",
        "      else:\n",
        "        all_predictions[example.qas_id] = best_non_null_entry.text\n",
        "\n",
        "    all_nbest_json[example.qas_id] = nbest_json\n",
        "\n",
        "  with tf.gfile.GFile(output_prediction_file, \"w\") as writer:\n",
        "    writer.write(json.dumps(all_predictions, indent=4) + \"\\n\")\n",
        "\n",
        "  with tf.gfile.GFile(output_nbest_file, \"w\") as writer:\n",
        "    writer.write(json.dumps(all_nbest_json, indent=4) + \"\\n\")\n",
        "\n",
        "  if FLAGS.version_2_with_negative:\n",
        "    with tf.gfile.GFile(output_null_log_odds_file, \"w\") as writer:\n",
        "      writer.write(json.dumps(scores_diff_json, indent=4) + \"\\n\")\n",
        "\n",
        "\n",
        "def get_final_text(pred_text, orig_text, do_lower_case):\n",
        "  \"\"\"Project the tokenized prediction back to the original text.\"\"\"\n",
        "\n",
        "  # When we created the data, we kept track of the alignment between original\n",
        "  # (whitespace tokenized) tokens and our WordPiece tokenized tokens. So\n",
        "  # now `orig_text` contains the span of our original text corresponding to the\n",
        "  # span that we predicted.\n",
        "  #\n",
        "  # However, `orig_text` may contain extra characters that we don't want in\n",
        "  # our prediction.\n",
        "  #\n",
        "  # For example, let's say:\n",
        "  #   pred_text = steve smith\n",
        "  #   orig_text = Steve Smith's\n",
        "  #\n",
        "  # We don't want to return `orig_text` because it contains the extra \"'s\".\n",
        "  #\n",
        "  # We don't want to return `pred_text` because it's already been normalized\n",
        "  # (the SQuAD eval script also does punctuation stripping/lower casing but\n",
        "  # our tokenizer does additional normalization like stripping accent\n",
        "  # characters).\n",
        "  #\n",
        "  # What we really want to return is \"Steve Smith\".\n",
        "  #\n",
        "  # Therefore, we have to apply a semi-complicated alignment heruistic between\n",
        "  # `pred_text` and `orig_text` to get a character-to-charcter alignment. This\n",
        "  # can fail in certain cases in which case we just return `orig_text`.\n",
        "\n",
        "  def _strip_spaces(text):\n",
        "    ns_chars = []\n",
        "    ns_to_s_map = collections.OrderedDict()\n",
        "    for (i, c) in enumerate(text):\n",
        "      if c == \" \":\n",
        "        continue\n",
        "      ns_to_s_map[len(ns_chars)] = i\n",
        "      ns_chars.append(c)\n",
        "    ns_text = \"\".join(ns_chars)\n",
        "    return (ns_text, ns_to_s_map)\n",
        "\n",
        "  # We first tokenize `orig_text`, strip whitespace from the result\n",
        "  # and `pred_text`, and check if they are the same length. If they are\n",
        "  # NOT the same length, the heuristic has failed. If they are the same\n",
        "  # length, we assume the characters are one-to-one aligned.\n",
        "  tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n",
        "\n",
        "  tok_text = \" \".join(tokenizer.tokenize(orig_text))\n",
        "\n",
        "  start_position = tok_text.find(pred_text)\n",
        "  if start_position == -1:\n",
        "    if FLAGS.verbose_logging:\n",
        "      tf.logging.info(\n",
        "          \"Unable to find text: '%s' in '%s'\" % (pred_text, orig_text))\n",
        "    return orig_text\n",
        "  end_position = start_position + len(pred_text) - 1\n",
        "\n",
        "  (orig_ns_text, orig_ns_to_s_map) = _strip_spaces(orig_text)\n",
        "  (tok_ns_text, tok_ns_to_s_map) = _strip_spaces(tok_text)\n",
        "\n",
        "  if len(orig_ns_text) != len(tok_ns_text):\n",
        "    if FLAGS.verbose_logging:\n",
        "      tf.logging.info(\"Length not equal after stripping spaces: '%s' vs '%s'\",\n",
        "                      orig_ns_text, tok_ns_text)\n",
        "    return orig_text\n",
        "\n",
        "  # We then project the characters in `pred_text` back to `orig_text` using\n",
        "  # the character-to-character alignment.\n",
        "  tok_s_to_ns_map = {}\n",
        "  for (i, tok_index) in six.iteritems(tok_ns_to_s_map):\n",
        "    tok_s_to_ns_map[tok_index] = i\n",
        "\n",
        "  orig_start_position = None\n",
        "  if start_position in tok_s_to_ns_map:\n",
        "    ns_start_position = tok_s_to_ns_map[start_position]\n",
        "    if ns_start_position in orig_ns_to_s_map:\n",
        "      orig_start_position = orig_ns_to_s_map[ns_start_position]\n",
        "\n",
        "  if orig_start_position is None:\n",
        "    if FLAGS.verbose_logging:\n",
        "      tf.logging.info(\"Couldn't map start position\")\n",
        "    return orig_text\n",
        "\n",
        "  orig_end_position = None\n",
        "  if end_position in tok_s_to_ns_map:\n",
        "    ns_end_position = tok_s_to_ns_map[end_position]\n",
        "    if ns_end_position in orig_ns_to_s_map:\n",
        "      orig_end_position = orig_ns_to_s_map[ns_end_position]\n",
        "\n",
        "  if orig_end_position is None:\n",
        "    if FLAGS.verbose_logging:\n",
        "      tf.logging.info(\"Couldn't map end position\")\n",
        "    return orig_text\n",
        "\n",
        "  output_text = orig_text[orig_start_position:(orig_end_position + 1)]\n",
        "  return output_text\n",
        "\n",
        "\n",
        "def _get_best_indexes(logits, n_best_size):\n",
        "  \"\"\"Get the n-best logits from a list.\"\"\"\n",
        "  index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "  best_indexes = []\n",
        "  for i in range(len(index_and_score)):\n",
        "    if i >= n_best_size:\n",
        "      break\n",
        "    best_indexes.append(index_and_score[i][0])\n",
        "  return best_indexes\n",
        "\n",
        "\n",
        "def _compute_softmax(scores):\n",
        "  \"\"\"Compute softmax probability over raw logits.\"\"\"\n",
        "  if not scores:\n",
        "    return []\n",
        "\n",
        "  max_score = None\n",
        "  for score in scores:\n",
        "    if max_score is None or score > max_score:\n",
        "      max_score = score\n",
        "\n",
        "  exp_scores = []\n",
        "  total_sum = 0.0\n",
        "  for score in scores:\n",
        "    x = math.exp(score - max_score)\n",
        "    exp_scores.append(x)\n",
        "    total_sum += x\n",
        "\n",
        "  probs = []\n",
        "  for score in exp_scores:\n",
        "    probs.append(score / total_sum)\n",
        "  return probs\n",
        "\n",
        "\n",
        "class FeatureWriter(object):\n",
        "  \"\"\"Writes InputFeature to TF example file.\"\"\"\n",
        "\n",
        "  def __init__(self, filename, is_training):\n",
        "    self.filename = filename\n",
        "    self.is_training = is_training\n",
        "    self.num_features = 0\n",
        "    self._writer = tf.python_io.TFRecordWriter(filename)\n",
        "\n",
        "  def process_feature(self, feature):\n",
        "    \"\"\"Write a InputFeature to the TFRecordWriter as a tf.train.Example.\"\"\"\n",
        "    self.num_features += 1\n",
        "\n",
        "    def create_int_feature(values):\n",
        "      feature = tf.train.Feature(\n",
        "          int64_list=tf.train.Int64List(value=list(values)))\n",
        "      return feature\n",
        "\n",
        "    features = collections.OrderedDict()\n",
        "    features[\"unique_ids\"] = create_int_feature([feature.unique_id])\n",
        "    features[\"input_ids\"] = create_int_feature(feature.input_ids)\n",
        "    features[\"input_mask\"] = create_int_feature(feature.input_mask)\n",
        "    features[\"segment_ids\"] = create_int_feature(feature.segment_ids)\n",
        "\n",
        "    if self.is_training:\n",
        "      features[\"start_positions\"] = create_int_feature([feature.start_position])\n",
        "      features[\"end_positions\"] = create_int_feature([feature.end_position])\n",
        "      impossible = 0\n",
        "      if feature.is_impossible:\n",
        "        impossible = 1\n",
        "      features[\"is_impossible\"] = create_int_feature([impossible])\n",
        "\n",
        "    tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
        "    self._writer.write(tf_example.SerializeToString())\n",
        "\n",
        "  def close(self):\n",
        "    self._writer.close()\n",
        "\n",
        "\n",
        "def validate_flags_or_throw(bert_config):\n",
        "  \"\"\"Validate the input FLAGS or throw an exception.\"\"\"\n",
        "  validate_case_matches_checkpoint(FLAGS.do_lower_case,\n",
        "                                                FLAGS.init_checkpoint)\n",
        "\n",
        "  if not FLAGS.do_train and not FLAGS.do_predict:\n",
        "    raise ValueError(\"At least one of `do_train` or `do_predict` must be True.\")\n",
        "\n",
        "  if FLAGS.do_train:\n",
        "    if not FLAGS.train_file:\n",
        "      raise ValueError(\n",
        "          \"If `do_train` is True, then `train_file` must be specified.\")\n",
        "  if FLAGS.do_predict:\n",
        "    if not FLAGS.predict_file:\n",
        "      raise ValueError(\n",
        "          \"If `do_predict` is True, then `predict_file` must be specified.\")\n",
        "\n",
        "  if FLAGS.max_seq_length > bert_config.max_position_embeddings:\n",
        "    raise ValueError(\n",
        "        \"Cannot use sequence length %d because the BERT model \"\n",
        "        \"was only trained up to sequence length %d\" %\n",
        "        (FLAGS.max_seq_length, bert_config.max_position_embeddings))\n",
        "\n",
        "  if FLAGS.max_seq_length <= FLAGS.max_query_length + 3:\n",
        "    raise ValueError(\n",
        "        \"The max_seq_length (%d) must be greater than max_query_length \"\n",
        "        \"(%d) + 3\" % (FLAGS.max_seq_length, FLAGS.max_query_length))\n",
        "\n",
        "\n",
        "def main(_):\n",
        "  tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "  bert_config = BertConfig.from_json_file(FLAGS.bert_config_file)\n",
        "\n",
        "  validate_flags_or_throw(bert_config)\n",
        "\n",
        "  tf.gfile.MakeDirs(FLAGS.output_dir)\n",
        "\n",
        "  tokenizer = FullTokenizer(\n",
        "      vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\n",
        "\n",
        "  tpu_cluster_resolver = None\n",
        "  if FLAGS.use_tpu and FLAGS.tpu_name:\n",
        "    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n",
        "        FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)\n",
        "\n",
        "  is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
        "  run_config = tf.contrib.tpu.RunConfig(\n",
        "      cluster=tpu_cluster_resolver,\n",
        "      master=FLAGS.master,\n",
        "      model_dir=FLAGS.output_dir,\n",
        "      save_checkpoints_steps=FLAGS.save_checkpoints_steps,\n",
        "      tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "          iterations_per_loop=FLAGS.iterations_per_loop,\n",
        "          num_shards=FLAGS.num_tpu_cores,\n",
        "          per_host_input_for_training=is_per_host))\n",
        "\n",
        "  train_examples = None\n",
        "  num_train_steps = None\n",
        "  num_warmup_steps = None\n",
        "  if FLAGS.do_train:\n",
        "    train_examples = read_squad_examples(\n",
        "        input_file=FLAGS.train_file, is_training=True)\n",
        "    num_train_steps = int(\n",
        "        len(train_examples) / FLAGS.train_batch_size * FLAGS.num_train_epochs)\n",
        "    num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion)\n",
        "\n",
        "    # Pre-shuffle the input to avoid having to make a very large shuffle\n",
        "    # buffer in in the `input_fn`.\n",
        "    rng = random.Random(12345)\n",
        "    rng.shuffle(train_examples)\n",
        "\n",
        "  model_fn = model_fn_builder(\n",
        "      bert_config=bert_config,\n",
        "      init_checkpoint=FLAGS.init_checkpoint,\n",
        "      learning_rate=FLAGS.learning_rate,\n",
        "      num_train_steps=num_train_steps,\n",
        "      num_warmup_steps=num_warmup_steps,\n",
        "      use_tpu=FLAGS.use_tpu,\n",
        "      use_one_hot_embeddings=FLAGS.use_tpu)\n",
        "\n",
        "  # If TPU is not available, this will fall back to normal Estimator on CPU\n",
        "  # or GPU.\n",
        "  estimator = tf.contrib.tpu.TPUEstimator(\n",
        "      use_tpu=FLAGS.use_tpu,\n",
        "      model_fn=model_fn,\n",
        "      config=run_config,\n",
        "      train_batch_size=FLAGS.train_batch_size,\n",
        "      predict_batch_size=FLAGS.predict_batch_size)\n",
        "\n",
        "  if FLAGS.do_train:\n",
        "    # We write to a temporary file to avoid storing very large constant tensors\n",
        "    # in memory.\n",
        "    train_writer = FeatureWriter(\n",
        "        filename=os.path.join(FLAGS.output_dir, \"train.tf_record\"),\n",
        "        is_training=True)\n",
        "    convert_examples_to_features(\n",
        "        examples=train_examples,\n",
        "        tokenizer=tokenizer,\n",
        "        max_seq_length=FLAGS.max_seq_length,\n",
        "        doc_stride=FLAGS.doc_stride,\n",
        "        max_query_length=FLAGS.max_query_length,\n",
        "        is_training=True,\n",
        "        output_fn=train_writer.process_feature)\n",
        "    train_writer.close()\n",
        "\n",
        "    tf.logging.info(\"***** Running training *****\")\n",
        "    tf.logging.info(\"  Num orig examples = %d\", len(train_examples))\n",
        "    tf.logging.info(\"  Num split examples = %d\", train_writer.num_features)\n",
        "    tf.logging.info(\"  Batch size = %d\", FLAGS.train_batch_size)\n",
        "    tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
        "    del train_examples\n",
        "\n",
        "    train_input_fn = input_fn_builder(\n",
        "        input_file=train_writer.filename,\n",
        "        seq_length=FLAGS.max_seq_length,\n",
        "        is_training=True,\n",
        "        drop_remainder=True)\n",
        "    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "\n",
        "  if FLAGS.do_predict:\n",
        "    eval_examples = read_squad_examples(\n",
        "        input_file=FLAGS.predict_file, is_training=False)\n",
        "\n",
        "    eval_writer = FeatureWriter(\n",
        "        filename=os.path.join(FLAGS.output_dir, \"eval.tf_record\"),\n",
        "        is_training=False)\n",
        "    eval_features = []\n",
        "\n",
        "    def append_feature(feature):\n",
        "      eval_features.append(feature)\n",
        "      eval_writer.process_feature(feature)\n",
        "\n",
        "    convert_examples_to_features(\n",
        "        examples=eval_examples,\n",
        "        tokenizer=tokenizer,\n",
        "        max_seq_length=FLAGS.max_seq_length,\n",
        "        doc_stride=FLAGS.doc_stride,\n",
        "        max_query_length=FLAGS.max_query_length,\n",
        "        is_training=False,\n",
        "        output_fn=append_feature)\n",
        "    eval_writer.close()\n",
        "\n",
        "    tf.logging.info(\"***** Running predictions *****\")\n",
        "    tf.logging.info(\"  Num orig examples = %d\", len(eval_examples))\n",
        "    tf.logging.info(\"  Num split examples = %d\", len(eval_features))\n",
        "    tf.logging.info(\"  Batch size = %d\", FLAGS.predict_batch_size)\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    predict_input_fn = input_fn_builder(\n",
        "        input_file=eval_writer.filename,\n",
        "        seq_length=FLAGS.max_seq_length,\n",
        "        is_training=False,\n",
        "        drop_remegybestainder=False)\n",
        "\n",
        "    # If running eval on the TPU, you will need to specify the number of\n",
        "    # steps.\n",
        "    all_results = []\n",
        "    for result in estimator.predict(\n",
        "        predict_input_fn, yield_single_examples=True):\n",
        "      if len(all_results) % 1000 == 0:\n",
        "        tf.logging.info(\"Processing example: %d\" % (len(all_results)))\n",
        "      unique_id = int(result[\"unique_ids\"])\n",
        "      start_logits = [float(x) for x in result[\"start_logits\"].flat]\n",
        "      end_logits = [float(x) for x in result[\"end_logits\"].flat]\n",
        "      all_results.append(\n",
        "          RawResult(\n",
        "              unique_id=unique_id,\n",
        "              start_logits=start_logits,\n",
        "              end_logits=end_logits))\n",
        "\n",
        "    output_prediction_file = os.path.join(FLAGS.output_dir, \"predictions.json\")\n",
        "    output_nbest_file = os.path.join(FLAGS.output_dir, \"nbest_predictions.json\")\n",
        "    output_null_log_odds_file = os.path.join(FLAGS.output_dir, \"null_odds.json\")\n",
        "\n",
        "    write_predictions(eval_examples, eval_features, all_results,\n",
        "                      FLAGS.n_best_size, FLAGS.max_answer_length,\n",
        "                      FLAGS.do_lower_case, output_prediction_file,\n",
        "                      output_nbest_file, output_null_log_odds_file)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29rjjcFuBHGm",
        "colab_type": "text"
      },
      "source": [
        "## run app"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYwdN8oo5fQL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  print(\"REACHED tf.app.run()\")\n",
        "  tf.app.run()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWYHIqgX56GI",
        "colab_type": "text"
      },
      "source": [
        "# EVAL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mWQKCbnFH8K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fixEncoding(predictions_file_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EaisJs557hC",
        "colab_type": "code",
        "outputId": "b108e50b-4ff1-4cea-d90c-70c06f08e41d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "from collections import Counter\n",
        "import string\n",
        "import re\n",
        "import argparse\n",
        "import json\n",
        "import sys\n",
        "import nltk\n",
        "import random\n",
        "nltk.download('punkt')\n",
        "def normalize_answer(s):\n",
        "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "\n",
        "def f1_score(prediction, ground_truth):\n",
        "    prediction_tokens = normalize_answer(prediction).split()\n",
        "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "    precision = 1.0 * num_same / len(prediction_tokens)\n",
        "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "\n",
        "def exact_match_score(prediction, ground_truth):\n",
        "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
        "\n",
        "\n",
        "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
        "    scores_for_ground_truths = []\n",
        "    for ground_truth in ground_truths:\n",
        "        score = metric_fn(prediction, ground_truth)\n",
        "        scores_for_ground_truths.append(score)\n",
        "    return max(scores_for_ground_truths)\n",
        "\n",
        "\n",
        "\n",
        "def evaluate(dataset, predictions):\n",
        "    f1 = exact_match = total = exact_sentence = inclusion = random = 0\n",
        "    for article in dataset:\n",
        "        for paragraph in article['paragraphs']:\n",
        "            for qa in paragraph['qas']:\n",
        "                total += 1\n",
        "                if qa['id'] not in predictions:\n",
        "                    message = 'Unanswered question ' + qa['id'] + \\\n",
        "                              ' will receive score 0.'\n",
        "                    print(message, file=sys.stderr)\n",
        "                    continue\n",
        "                ground_truths = list(map(lambda x: x['text'], qa['answers']))\n",
        "                prediction = predictions[qa['id']]\n",
        "                sents = nltk.sent_tokenize(paragraph['context'])\n",
        "                indx_g = -1\n",
        "                indx_p = -1\n",
        "                i = 0\n",
        "                for sent in sents:\n",
        "                    if sent.find(ground_truths[0]) != -1:\n",
        "                        indx_g = i\n",
        "                    if sent.find(prediction) != -1:\n",
        "                        indx_p = i\n",
        "                    i += 1\n",
        "                if prediction.find(ground_truths[0]) != -1 or ground_truths[0].find(prediction):\n",
        "                    inclusion += 1\n",
        "                if indx_g == indx_p and indx_p != -1:\n",
        "                    exact_sentence += 1\n",
        "                exact_match += metric_max_over_ground_truths(\n",
        "                    exact_match_score, prediction, ground_truths)\n",
        "                f1 += metric_max_over_ground_truths(\n",
        "                    f1_score, prediction, ground_truths)\n",
        "    exact_sentence = 100 * exact_sentence / total\n",
        "    exact_match = 100.0 * exact_match / total\n",
        "    f1 = 100.0 * f1 / total\n",
        "\n",
        "    return {'exact_match': exact_match, 'f1': f1, 'exact_sentence': exact_sentence}\n",
        "\n",
        "#evaluation\n",
        "with open(predict_file_path) as dataset_file:\n",
        "  dataset_json = json.load(dataset_file)\n",
        "  dataset = dataset_json['data']\n",
        "with open(predictions_file_path) as prediction_file:\n",
        "  predictions = json.load(prediction_file)\n",
        "print(json.dumps(evaluate(dataset, predictions)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "{\"exact_match\": 16.129032258064516, \"f1\": 46.33307711272504, \"exact_sentence\": 88.17204301075269}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyHloYE-9edI",
        "colab_type": "text"
      },
      "source": [
        "# change bert checkpoint file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk1g-V0B7CeI",
        "colab_type": "text"
      },
      "source": [
        "## show current checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-B4Pvl16gbS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%pycat checkpoint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0dzVrZL7Gaw",
        "colab_type": "text"
      },
      "source": [
        "## change checkpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8DU6_Xb7h8w",
        "colab_type": "text"
      },
      "source": [
        "### write checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4OA9Csp9kRf",
        "colab_type": "code",
        "outputId": "165da01c-1161-42ca-df7c-1b474cb8b0ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#add checkpoint for pausing and resuming the training  here we set the model checkpoint to 543\n",
        "%%writefile checkpoint\n",
        "model_checkpoint_path: \"model.ckpt-543\"\n",
        "all_model_checkpoint_paths: \"model.ckpt-543\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing checkpoint\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mz4XXIM_7m0P",
        "colab_type": "text"
      },
      "source": [
        "### move check point to the model directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkQIfH7l7Xxa",
        "colab_type": "code",
        "outputId": "786f27ec-c669-490e-af8d-3bf08a2d5cda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "!mv checkpoint $output_dir_command"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert_config.json                     bert_model.ckpt.index  checkpoint\n",
            "bert_model.ckpt.data-00000-of-00001  bert_model.ckpt.meta   vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}